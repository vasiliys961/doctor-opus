"""
Vision –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–°–û–î–ï–†–ñ–ò–¢ –í–°–Æ –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–£–Æ –õ–û–ì–ò–ö–£ –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô!

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–æ–¥ send_vision_request, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –¢–û–ß–ù–û–ô –ö–û–ü–ò–ï–ô
–ª–æ–≥–∏–∫–∏ –∏–∑ claude_assistant.py. –í—Å—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
"""

import time
import requests
import json
from typing import Optional, List, Dict, Any

from .base_client import BaseAPIClient
from .diagnostic_prompts import get_system_prompt, get_diagnostic_prompt
from .model_router import select_models_list_for_diagnosis, detect_request_type
from .logging_handler import log_api_error, log_api_success, _get_model_name

# –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
from utils.error_handler import handle_error, log_api_call
from utils.performance_monitor import track_model_usage
from utils.cache_manager import get_image_hash, get_cache_key, get_cached_result, save_to_cache, clear_old_cache

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ claude_assistant.py (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ —Ç–µ–∫—É—â–∏–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤)
API_TIMEOUT_SECONDS = 120
API_TIMEOUT_LONG_SECONDS = 180
MAX_TOKENS_ECG = 3200
MAX_TOKENS_DEFAULT = 4000
MAX_TOKENS_ECG_LIST = [2600, 2000, 1500]
MAX_TOKENS_DEFAULT_LIST = [3000, 2000, 1000]
MAX_TOKENS_LLAMA = 1000
EXTENDED_THINKING_BUDGET = 10000
MIN_CONSENSUS_RESULTS = 2
MAX_CONSENSUS_MODELS = 4

# –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–æ–¥–µ–ª–∏ (–¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py)
DEPRECATED_MODELS = {
    'claude-3-sonnet': '–£—Å—Ç–∞—Ä–µ–ª–∞, 404',
    'gemini-pro-vision': '–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, 400',
    'qwen2-vl-72b': '–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, 400',
    'claude-3.5-sonnet': '–ó–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Sonnet 4.5',
    'claude-3-haiku': '–ó–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Haiku 4.5',
    'anthropic/claude-3-sonnet-20240229': '–£—Å—Ç–∞—Ä–µ–ª–∞, –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Sonnet 4.5',
    'anthropic/claude-3-5-sonnet-20241022': '–ó–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Sonnet 4.5',
    'anthropic/claude-3-5-sonnet': '–ó–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Sonnet 4.5',
    'anthropic/claude-3-haiku': '–ó–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Haiku 4.5',
    'google/gemini-pro-vision': '–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, 400',
    'qwen/qwen2-vl-72b-instruct': '–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, 400'
}

import logging

def check_deprecated(model_name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å —É—Å—Ç–∞—Ä–µ–≤—à–µ–π - –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py"""
    for deprecated, reason in DEPRECATED_MODELS.items():
        if deprecated in model_name.lower():
            logging.warning(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å—Ç–∞—Ä–µ–ª–∞: {reason}")
            return True
    return False


class VisionClient(BaseAPIClient):
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í—Å—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!
    –ú–µ—Ç–æ–¥ send_vision_request —è–≤–ª—è–µ—Ç—Å—è –¢–û–ß–ù–û–ô –ö–û–ü–ò–ï–ô –∏–∑ claude_assistant.py
    """
    
    # –§–ª–∞–≥ –∫–ª–∞—Å—Å–∞ –¥–ª—è –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —Ä–æ—É—Ç–µ—Ä–µ
    _router_warning_shown = False
    
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1/chat/completions"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vision –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            api_key: API –∫–ª—é—á OpenRouter
            base_url: –ë–∞–∑–æ–≤—ã–π URL API
        """
        super().__init__(api_key, base_url)
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
        self.system_prompt = get_system_prompt()
        
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: Claude 4.5 —Å–µ—Ä–∏—è + Llama
        # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–∞–≤–∏–ª–æ (–ù–ï –ò–ó–ú–ï–ù–Ø–ï–¢–°–Ø):
        # - –í—Å–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Üí Opus 4.5
        # - Fallback ‚Üí Sonnet 4.5 (–±—ã—Å—Ç—Ä–µ–µ Opus, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ Haiku)
        # - –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ/—Ä–∞–∑–±–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Üí Haiku 4.5
        self.models = [
            "anthropic/claude-opus-4.5",                # Opus 4.5 ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç (text + vision)
            "anthropic/claude-sonnet-4.5",              # Sonnet 4.5 ‚Äî –±—ã—Å—Ç—Ä—ã–π fallback (–ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞)
            "anthropic/claude-haiku-4.5",               # Haiku 4.5 ‚Äî –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/OCR
            "meta-llama/llama-3.2-90b-vision-instruct"  # Llama 3.2 90B Vision ‚Äî —Ä–µ–∑–µ—Ä–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        ]
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º Opus –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
        self.model = self.models[0]
    
    def _select_diagnostic_prompt(self, prompt: str, prompt_lower: str, metadata: Optional[dict], base_prompt: Optional[str] = None) -> str:
        """
        –í—ã–±–æ—Ä –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–ê–Ø –õ–û–ì–ò–ö–ê!
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –ª–æ–≥–∏–∫–∏ –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 351-846)
        –ù–ï –ò–ó–ú–ï–ù–Ø–¢–¨ –õ–û–ì–ò–ö–£ –í–´–ë–û–†–ê –ü–†–û–ú–ü–¢–û–í!
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            prompt_lower: –ü—Ä–æ–º–ø—Ç –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞
        
        Returns:
            str: –ü–æ–ª–Ω—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
        """
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –≤—Å—Ç–∞–≤–ª—è–µ–º —Å—Ä–∞–∑—É, –¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        base_prompt = base_prompt or self.system_prompt

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º ¬´—Ç–æ–ª—å–∫–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ¬ª (OCR/–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö),
        # –∫–æ–≥–¥–∞ –Ω–∞–º –ù–ï –Ω—É–∂–Ω–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞ –∏ –æ–±—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        scan_only_mode = isinstance(metadata, dict) and metadata.get("task") in ("lab_ocr", "doc_ocr")
        
        if scan_only_mode:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ system_prompt
            return prompt
        
        # –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –ª–æ–≥–∏–∫–∏ –≤—ã–±–æ—Ä–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ claude_assistant.py
        # –≠–ö–ì - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ô –ü–†–û–ú–ü–¢!
        if "—ç–∫–≥" in prompt_lower or "ecg" in prompt_lower:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≠–ö–ì (–∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ)
            optimized_description_mode = isinstance(metadata, dict) and metadata.get("task") in (
                "ecg_description_opus_optimized", 
                "ecg_description_fast_mode",
                "ecg_description_experimental",
                "ecg_description_gemini3_opus"
            )
            
            if optimized_description_mode:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ system_prompt –∏ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞)
                # –≠—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç —Ç–æ–∫–µ–Ω—ã –ø—Ä–∏ –¥–≤—É—Ö—à–∞–≥–æ–≤–æ–º –∞–Ω–∞–ª–∏–∑–µ
                print("‚úÖ [ECG PROMPT] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è")
                return prompt
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            try:
                from prompts.diagnostic_prompts import get_ecg_diagnostic_prompt
                # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≠–ö–ì —Ç–µ–ø–µ—Ä—å –ù–ï –≤–∫–ª—é—á–∞–µ—Ç system_prompt
                medical_prompt = get_ecg_diagnostic_prompt()
                print("‚úÖ [ECG PROMPT] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ diagnostic_prompts.py")
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
                return medical_prompt
            except (ImportError, Exception) as e:
                print(f"‚ö†Ô∏è [ECG PROMPT] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≠–ö–ì –±–µ–∑ system_prompt
                from prompts.diagnostic_prompts import get_ecg_diagnostic_prompt as _fallback_ecg_prompt
                return _fallback_ecg_prompt()
        
        # –†–µ–Ω—Ç–≥–µ–Ω - –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ
        elif "—Ä–µ–Ω—Ç–≥–µ–Ω" in prompt_lower or "xray" in prompt_lower or "–≥—Ä—É–¥–Ω" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_xray_diagnostic_prompt
                medical_prompt = get_xray_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
                return medical_prompt
            except ImportError:
                # Fallback - –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 444-715)
                # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω—ã–π fallback –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–µ–Ω—Ç–≥–µ–Ω–∞
                # –î–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ –æ–ø—É—Å–∫–∞—é, –Ω–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                return f"{base_prompt}\n\n{prompt}"
        
        # –ú–†–¢
        elif "–º—Ä—Ç" in prompt_lower or "mri" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_mri_diagnostic_prompt
                medical_prompt = get_mri_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
                return medical_prompt
            except ImportError:
                return f"""{base_prompt}

–í—ã ‚Äî –≤—Ä–∞—á-–Ω–µ–π—Ä–æ—Ä–∞–¥–∏–æ–ª–æ–≥ —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–ª–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–≤–∏–¥–µ–Ω–Ω–æ–≥–æ.

{prompt}"""
        
        # –ö–¢
        elif "–∫—Ç" in prompt_lower or "ct" in prompt_lower or "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_ct_diagnostic_prompt
                medical_prompt = get_ct_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
                return medical_prompt
            except ImportError:
                return f"""{base_prompt}

–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–∞–¥–∏–æ–ª–æ–≥, –æ–±–ª–∞–¥–∞–µ—à—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –ö–¢. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≤—ã—è–≤–ª—è—Ç—å –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –¥–∞–≤–∞—Ç—å –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.

{prompt}"""
        
        # –£–ó–ò
        elif "—É–∑–∏" in prompt_lower or "—É–ª—å—Ç—Ä–∞–∑–≤—É–∫" in prompt_lower or "ultrasound" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_ultrasound_diagnostic_prompt
                medical_prompt = get_ultrasound_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
                return medical_prompt
            except ImportError:
                return f"""{base_prompt}

–í—ã ‚Äî –≤—Ä–∞—á —É–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å 12-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º —Ä–∞–±–æ—Ç—ã. –î–µ—Ç–∞–ª—å–Ω–æ –æ–ø–∏—à–∏—Ç–µ –£–ó–ò-–∫–∞—Ä—Ç–∏–Ω—É.

{prompt}"""
        
        # –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
        elif "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä" in prompt_lower or ("–∞–Ω–∞–ª–∏–∑" in prompt_lower and ("–∫—Ä–æ–≤" in prompt_lower or "–º–æ—á" in prompt_lower or "–±–∏–æ—Ö–∏–º" in prompt_lower or "lab" in prompt_lower)):
            return f"""{base_prompt}

–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ.
–í –≠–¢–û–ú –†–ï–ñ–ò–ú–ï –¢–í–û–Ø –ó–ê–î–ê–ß–ê ‚Äî –¢–û–õ–¨–ö–û –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò–ó –õ–ê–ë–û–†–ê–¢–û–†–ù–û–ì–û –û–¢–ß–Å–¢–ê (CBC, –±–∏–æ—Ö–∏–º–∏—è –∏ –¥—Ä.).

–°–î–ï–õ–ê–ô –¢–û–õ–¨–ö–û –°–õ–ï–î–£–Æ–©–ï–ï:
- –ê–∫–∫—É—Ä–∞—Ç–Ω–æ –∏–∑–≤–ª–µ–∫–∏ –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–∑–≤–∞–Ω–∏–µ, –∑–Ω–∞—á–µ–Ω–∏–µ, –µ–¥–∏–Ω–∏—Ü—ã, —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, –µ—Å–ª–∏ –µ—Å—Ç—å).
- –ß—ë—Ç–∫–æ –æ—Ç–º–µ—Ç—å, –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤–Ω–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (–ø–æ–≤—ã—à–µ–Ω—ã/–ø–æ–Ω–∏–∂–µ–Ω—ã).
- –°–æ—Ö—Ä–∞–Ω–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, WBC, RBC, Hb, PLT, Lym%, Neu% –∏ —Ç.–ø.).

–í–ê–ñ–ù–û:
- –ù–ï –¥–∞–≤–∞–π –∫–ª–∏–Ω–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é, –¥–∏–∞–≥–Ω–æ–∑—ã, –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–∏–∞–≥–Ω–æ–∑ –∏–ª–∏ –ø–ª–∞–Ω –ª–µ—á–µ–Ω–∏—è.
- –ù–ï —Å—Ä–∞–≤–Ω–∏–≤–∞–π —ç—Ç–æ—Ç –æ—Ç—á—ë—Ç —Å —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏ –∏ –Ω–µ –ø–∏—à–∏, —á—Ç–æ —ç—Ç–æ ¬´–Ω–µ —Ä–µ–Ω—Ç–≥–µ–Ω / –Ω–µ –ú–†–¢ / –Ω–µ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ¬ª.
- –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–¥–µ–ª—ã –≤—Ä–æ–¥–µ ¬´–õ–æ–≥ –≤–µ–±-–∑–∞–ø—Ä–æ—Å–æ–≤¬ª, –Ω–µ –ø–µ—Ä–µ—á–∏—Å–ª—è–π —Å–∞–π—Ç—ã, URL, DOI ‚Äî —Å—Å—ã–ª–∫–∏ –∏ –ª–æ–≥ –∑–∞–ø—Ä–æ—Å–æ–≤ –ù–ï –ù–£–ñ–ù–´.
- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –∏–ª–∏ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–æ –±–µ–∑ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–∏–º–µ—Ä JSON –ø–æ –∂–µ–ª–∞–Ω–∏—é):
{{
  "parameters": [
    {{"name": "WBC", "value": "...", "unit": "...", "reference": "..." , "status": "high/low/normal"}},
    ...
  ],
  "raw_text": "–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ‚Äî –≤–µ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç—á—ë—Ç–∞"
}}"""
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã
        elif any(keyword in prompt_lower for keyword in {
            "–¥–æ–∫—É–º–µ–Ω—Ç", "—Å–ø—Ä–∞–≤–∫–∞", "—Ä–µ—Ü–µ–ø—Ç", "–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–≤—ã–ø–∏—Å–∫–∞", 
            "–±–æ–ª—å–Ω–∏—á–Ω—ã–π", "–∏–∑–≤–ª–µ–∫–∏—Ç–µ", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "document", "extract",
            "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å–ø—Ä–∞–≤–∫–∏", "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö"
        }):
            # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –ë–ï–ó system_prompt - —Ç–æ–ª—å–∫–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            return prompt
        
        # –û–±—â–∏–π —Å–ª—É—á–∞–π
        else:
            return f"""{base_prompt}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –≤—Ä–∞—á-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —Å –±–æ–ª—å—à–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã.
–î–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞¬ª.

{prompt}
"""
    
    def send_vision_request(
        self,
        prompt: str,
        image_array=None,
        metadata=None,
        use_cache: bool = False,
        use_router: bool = True,
        force_model: Optional[str] = None
    ) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å Vision –º–æ–¥–µ–ª—è–º–∏ - –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –õ–û–ì–ò–ö–ò –∏–∑ claude_assistant.py
        
        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í—Å—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            image_array: –ú–∞—Å—Å–∏–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∫–µ—à (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False - –∫–µ—à –æ—Ç–∫–ª—é—á–µ–Ω)
            use_router: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
            force_model: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ ('opus'/'sonnet'/'haiku'/'llama'/None)
        
        Returns:
            str: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        # –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –ª–æ–≥–∏–∫–∏ –∏–∑ claude_assistant.py send_vision_request (—Å—Ç—Ä–æ–∫–∏ 267-1158)
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞
        clear_old_cache()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_lower = prompt.lower() if prompt else ""
        
        # –î–ª—è –≠–ö–ì –∫—ç—à –≤—Å–µ–≥–¥–∞ –æ—Ç–∫–ª—é—á–µ–Ω
        is_ecg = "—ç–∫–≥" in prompt_lower or "ecg" in prompt_lower
        if is_ecg:
            use_cache = False
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–µ—à–∞ (–∏—Å–ø–æ–ª—å–∑—É—è —Ç—É –∂–µ –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ models_to_try[0])
        primary_model_for_cache = None
        if use_cache and image_array is not None and not is_ecg:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏—Ö
            if not hasattr(self, '_cached_active_models') or self._cached_active_models is None:
                self._cached_active_models = [m for m in self.models if not check_deprecated(m)]
            active_models = self._cached_active_models
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
            is_document, is_lab = detect_request_type(prompt_lower, metadata)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–µ—à–∞
            if force_model:
                fm = force_model.lower()
                if fm == "opus":
                    primary_model_for_cache = "anthropic/claude-opus-4.5"
                elif fm == "sonnet":
                    primary_model_for_cache = "anthropic/claude-sonnet-4.5"
                elif fm == "haiku":
                    primary_model_for_cache = "anthropic/claude-haiku-4.5"
                elif fm == "llama":
                    primary_model_for_cache = "meta-llama/llama-3.2-90b-vision-instruct"
                else:
                    primary_model_for_cache = active_models[0] if active_models else self.models[0]
            elif is_document:
                primary_model_for_cache = "anthropic/claude-haiku-4.5"
            elif is_lab:
                primary_model_for_cache = "anthropic/claude-sonnet-4.5"
            else:
                primary_model_for_cache = "anthropic/claude-opus-4.5"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ use_cache=True –∏ —ç—Ç–æ –Ω–µ –≠–ö–ì)
        if use_cache and image_array is not None and not is_ecg:
            image_hash = get_image_hash(image_array)
            cache_key = get_cache_key(prompt, image_hash, primary_model_for_cache)
            cached_result = get_cached_result(cache_key, max_age_hours=24)
            
            if cached_result and cached_result.get('result'):
                print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª—É—á–µ–Ω –∏–∑ –∫—ç—à–∞")
                return cached_result['result']
        
        # –í—ã–±–æ—Ä –ø—Ä–æ–º–ø—Ç–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–ê–Ø –õ–û–ì–ò–ö–ê!
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞,
        # –∑–∞—Ç–µ–º –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
        base_prompt = self.system_prompt
        medical_prompt = self._select_diagnostic_prompt(prompt, prompt_lower, metadata, base_prompt)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        content = [{"type": "text", "text": medical_prompt}]
        
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{metadata_str}"})
        
        if image_array is not None:
            base64_str = self.encode_image(image_array)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_str}"}
            })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∏—Ö
        if not hasattr(self, '_cached_active_models') or self._cached_active_models is None:
            self._cached_active_models = [m for m in self.models if not check_deprecated(m)]
        active_models = self._cached_active_models
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        is_document, is_lab = detect_request_type(prompt_lower, metadata)
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º model_router
        models_to_try = select_models_list_for_diagnosis(
            prompt_lower,
            force_model,
            is_document,
            is_lab,
            active_models
        )
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω –∫–æ–Ω—Å–µ–Ω—Å—É—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π
        use_consensus = False
        if isinstance(metadata, dict):
            use_consensus = metadata.get('consensus_mode', False)
        
        # –î–ª—è –≠–ö–ì –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è
        max_tokens_consensus = MAX_TOKENS_ECG if is_ecg else MAX_TOKENS_DEFAULT
        
        if use_consensus and len(models_to_try) > 1:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 3-4 –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞
            models_to_try = models_to_try[:min(MAX_CONSENSUS_MODELS, len(models_to_try))]
            results = []
            min_consensus_results = min(MIN_CONSENSUS_RESULTS, len(models_to_try))
            
            for model in models_to_try:
                try:
                    start_time = time.time()
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": content}
                    ]
                    payload = {
                        "model": model,
                        "messages": messages,
                        "max_tokens": max_tokens_consensus,
                        "temperature": 0.1
                    }
                    
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=API_TIMEOUT_SECONDS)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        result = result_data["choices"][0]["message"]["content"]
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        log_api_success(model, latency, tokens_used)
                        results.append({
                            "model": model,
                            "result": result,
                            "tokens": tokens_used
                        })
                        if len(results) >= min_consensus_results:
                            break
                    elif response.status_code == 402:
                        print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –¥–ª—è {max_tokens_consensus} —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–µ. –ü—Ä–æ–ø—É—Å–∫–∞—é –º–æ–¥–µ–ª—å {model}.")
                        error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤"
                        log_api_error(model, latency, error_msg)
                        continue
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_error(model, latency, error_msg)
                except Exception as e:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = handle_error(e, f"send_vision_request ({model})", show_to_user=False)
                    log_api_error(model, latency, error_msg)
                    continue
            
            if results:
                return results
        
        # –î–ª—è –≠–ö–ì –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è
        max_tokens_list = MAX_TOKENS_ECG_LIST if is_ecg else MAX_TOKENS_DEFAULT_LIST
        
        # Fallback –º–æ–¥–µ–ª–∏
        claude_failed = False
        fallback_models = []
        if is_document or (force_model and force_model.lower() == "llama"):
            fallback_models = []
        elif is_ecg:
            fallback_models = ["anthropic/claude-opus-4.5", "meta-llama/llama-3.2-90b-vision-instruct"]
        else:
            fallback_models = [m for m in active_models if m not in models_to_try]
            if "meta-llama/llama-3.2-90b-vision-instruct" not in fallback_models:
                fallback_models.append("meta-llama/llama-3.2-90b-vision-instruct")
        
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - –ø—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É
        for model in models_to_try:
            for max_tokens in max_tokens_list:
                try:
                    start_time = time.time()
                    model_name = _get_model_name(model)
                    print(f"ü§ñ [{model_name}] –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (max_tokens={max_tokens})...")
                    
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": content}
                    ]
                    payload = {
                        "model": model,
                        "messages": messages,
                        "max_tokens": max_tokens,
                        "temperature": 0.1
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Claude 4.5 –º–æ–¥–µ–ª–µ–π
                    if isinstance(metadata, dict):
                        if 'claude-opus-4.5' in model and 'verbosity' in metadata.get('model_params', {}):
                            payload['verbosity'] = metadata['model_params']['verbosity']
                        
                        if any(x in model for x in ['claude-sonnet-4.5', 'claude-haiku-4.5']):
                            if metadata.get('model_params', {}).get('extended_thinking', False):
                                payload['thinking'] = {
                                    "type": "enabled",
                                    "budget_tokens": EXTENDED_THINKING_BUDGET
                                }
                    
                    print(f"üì° [{model_name}] –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ API...")
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=API_TIMEOUT_SECONDS)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        result = result_data["choices"][0]["message"]["content"]
                        
                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
                        if use_cache and image_array is not None and not is_ecg:
                            image_hash = get_image_hash(image_array)
                            cache_key = get_cache_key(prompt, image_hash, primary_model_for_cache)
                            save_to_cache(cache_key, result, max_age_hours=24)
                        
                        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        input_tokens = result_data.get("usage", {}).get("prompt_tokens", tokens_used // 2)
                        output_tokens = result_data.get("usage", {}).get("completion_tokens", tokens_used // 2)
                        model_name = _get_model_name(model)
                        log_api_success(model, latency, tokens_used, f"{model_name}")
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        try:
                            import streamlit as st
                            st.session_state.last_request_info = {
                                'model': model,
                                'tokens': tokens_used,
                                'input_tokens': input_tokens,
                                'output_tokens': output_tokens,
                                'latency': latency
                            }
                        except:
                            pass
                        
                        # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å
                        if is_document or (force_model and force_model.lower() == "llama"):
                            return result
                        return f"**ü©∫ –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ ({model_name}):**\n\n{result}"
                    elif response.status_code == 402:
                        if max_tokens == max_tokens_list[-1]:
                            claude_failed = True
                            print(f"‚ùå [{model_name}] –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞). –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ fallback –º–æ–¥–µ–ª—å...")
                            break
                        else:
                            print(f"‚ö†Ô∏è [{model_name}] –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –¥–ª—è {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤. –ü—Ä–æ–±—É—é –º–µ–Ω—å—à–µ...")
                            continue
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_error(model, latency, error_msg, "VISION REQUEST")
                        break
                        
                except Exception as e:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = handle_error(e, f"send_vision_request ({model})", show_to_user=False)
                    log_api_error(model, latency, error_msg)
                    continue
            
            if claude_failed:
                break
        
        # Fallback –Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
        if claude_failed and fallback_models:
            print(f"üîÑ [FALLBACK] –í—Å–µ Claude –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü—Ä–æ–±—É—é fallback –º–æ–¥–µ–ª–∏: {', '.join([_get_model_name(m) for m in fallback_models])}")
            for model in fallback_models:
                try:
                    start_time = time.time()
                    model_name = _get_model_name(model)
                    print(f"ü§ñ [FALLBACK {model_name}] –ü—Ä–æ–±—É—é fallback –º–æ–¥–µ–ª—å...")
                    
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": content}
                    ]
                    payload = {
                        "model": model,
                        "messages": messages,
                        "max_tokens": MAX_TOKENS_LLAMA,
                        "temperature": 0.1
                    }
                    
                    print(f"üì° [FALLBACK {model_name}] –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ API...")
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=API_TIMEOUT_SECONDS)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        result = result_data["choices"][0]["message"]["content"]
                        
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        model_name = _get_model_name(model)
                        log_api_success(model, latency, tokens_used, f"FALLBACK {model_name}")
                        
                        if is_document or (force_model and force_model.lower() == "llama"):
                            return result
                        return f"**ü©∫ –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ ({model_name}) [Fallback]:**\n\n{result}"
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_error(model, latency, error_msg)
                        continue
                        
                except Exception as e:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = handle_error(e, f"send_vision_request fallback ({model})", show_to_user=False)
                    log_api_error(model, latency, error_msg)
                    continue
        
        return "‚ùå –û—à–∏–±–∫–∞: –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
    
    def send_vision_request_gemini_fast(self, prompt: str, image_array=None, metadata=None, use_flash_3: bool = False):
        """
        –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Gemini Flash (2.5 –∏–ª–∏ 3.0)
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            image_array: –ú–∞—Å—Å–∏–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            use_flash_3: –ï—Å–ª–∏ True, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini 3.0 Flash, –∏–Ω–∞—á–µ 2.5 Flash
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç Gemini Flash
        """
        # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        if use_flash_3:
            models_to_try = [
                "google/gemini-3-flash-preview",      # Flash 3.0 Preview (–∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ OpenRouter)
                "google/gemini-3-flash",               # Flash 3.0 (–µ—Å–ª–∏ –ø–æ—è–≤–∏—Ç—Å—è –±–µ–∑ preview)
                "google/gemini-2.5-flash"              # Fallback –Ω–∞ Flash 2.5
            ]
        else:
            models_to_try = ["google/gemini-2.5-flash"]
        
        model = models_to_try[0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞
        
        print(f"ü§ñ [‚ö° FLASH] [GEMINI FLASH] –ù–∞—á–∏–Ω–∞—é –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        
        prompt_lower = prompt.lower() if prompt else ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫–∞–∫ —É Opus, –Ω–æ –±–µ–∑ system_prompt)
        if "—ç–∫–≥" in prompt_lower or "ecg" in prompt_lower:
            medical_prompt = f"""–¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –∫–∞—Ä–¥–∏–æ–ª–æ–≥-—ç–ª–µ–∫—Ç—Ä–æ—Ñ–∏–∑–∏–æ–ª–æ–≥ (board certified). 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ 12‚Äë–∫–∞–Ω–∞–ª—å–Ω–æ–π –≠–ö–ì (–≤–∫–ª—é—á–∞—è —Å–ª–æ–∂–Ω—ã–µ –∞—Ä–∏—Ç–º–∏–∏ –∏ –±–ª–æ–∫–∞–¥—ã)
–∏ –≤—ã–¥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞¬ª.

üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û ‚Äî –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –°–ò–ì–ù–ê–õ–ê:
–°–ù–ê–ß–ê–õ–ê –æ—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∏ –≠–ö–ì:
- –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –û–¢–õ–ò–ß–ù–û–ï –∏–ª–∏ –•–û–†–û–®–ï–ï ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
- –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –ü–õ–û–•–û–ï –∏–ª–∏ –û–ß–ï–ù–¨ –ü–õ–û–•–û–ï (–º–Ω–æ–≥–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, —à—É–º–∞, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞–∑–ª–∏—á–∏—Ç—å —á–µ—Ç–∫–∏–µ P, QRS, T –≤–æ–ª–Ω—ã) ‚Äî 
  –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —É–∫–∞–∂–∏ —ç—Ç–æ –≤ –Ω–∞—á–∞–ª–µ: "‚ö†Ô∏è –ö–ê–ß–ï–°–¢–í–û –ó–ê–ü–ò–°–ò: –ü–õ–û–•–û–ï/–û–ß–ï–ù–¨ –ü–õ–û–•–û–ï. –ú–Ω–æ–≥–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏ —à—É–º–∞, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ —Ä–∞–∑–ª–∏—á–∏—Ç—å –∑—É–±—Ü—ã P, QRS, T. 
  –ê–Ω–∞–ª–∏–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω/–Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å –≠–ö–ì."
- –ï—Å–ª–∏ –≠–ö–ì –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ—á–∏—Ç–∞–µ–º–æ ‚Äî —á–µ—Å—Ç–Ω–æ —É–∫–∞–∂–∏ —ç—Ç–æ –∏ –Ω–µ –ø—ã—Ç–∞–π—Å—è –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ, —á—Ç–æ –Ω–µ –≤–∏–¥–Ω–æ.

–ò–≥–Ω–æ—Ä–∏—Ä—É–π —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ —Ç–∞–±–ª–∏—Ü–∞—Ö —Å—Å—ã–ª–æ–∫ –∏ –ª–æ–≥–∞—Ö –≤–µ–±‚Äë–ø–æ–∏—Å–∫–∞: —Å—Å—ã–ª–∫–∏ –∏ –ª–æ–≥–∏ –ù–ï –ù–£–ñ–ù–´ –≤ –æ—Ç–≤–µ—Ç–µ.
–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–∫–∏/—Å—Ç–æ–ª–±—Ü—ã —Å ¬´–ü–∞—Ä–∞–º–µ—Ç—Ä / –ó–Ω–∞—á–µ–Ω–∏–µ¬ª); –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø–∏—Å—ã–≤–∞–π –≤ –≤–∏–¥–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–∏—Å–∫–æ–≤.

–í–ê–ñ–ù–û: –î–µ–ª–∞–π –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ö–û–ú–ü–ê–ö–¢–ù–´–ú. –ù–ï –ø–µ—Ä–µ—á–∏—Å–ª—è–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç–æ–ª–æ–≥–∏–∏. –£–∫–∞–∑—ã–≤–∞–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è.

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ü–†–û–í–ï–î–ò –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–ó:

1. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ò –ö–ê–ß–ï–°–¢–í–û –ó–ê–ü–ò–°–ò:
   - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Å–Ω–∞—á–∞–ª–∞ –æ—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞ (–æ—Ç–ª–∏—á–Ω–æ–µ/—Ö–æ—Ä–æ—à–µ–µ/—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ/–ø–ª–æ—Ö–æ–µ/–æ—á–µ–Ω—å –ø–ª–æ—Ö–æ–µ).
   - –§–æ—Ä–º–∞—Ç –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∏, –Ω–∞–ª–∏—á–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏ –ø–æ–º–µ—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å ‚Äî –æ–ø–∏—à–∏ –≤ –∫–∞–∫–∏—Ö –æ—Ç–≤–µ–¥–µ–Ω–∏—è—Ö –∏ –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤—ã—Ä–∞–∂–µ–Ω—ã).
   - –°–∫–æ—Ä–æ—Å—Ç—å –ª–µ–Ω—Ç—ã (25/50 –º–º/—Å) –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ (1 –º–í = 10 –º–º), –µ—Å–ª–∏ —ç—Ç–æ –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å.
   - –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø–ª–æ—Ö–æ–µ ‚Äî —É–∫–∞–∂–∏, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω–µ –≤–∏–¥–Ω–æ –∏–ª–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ, –∏ —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–ø–∏—Å—å.

2. –†–ò–¢–ú –ò –ü–†–û–í–û–î–ò–ú–û–°–¢–¨:
   - –û—Å–Ω–æ–≤–Ω–æ–π —Ä–∏—Ç–º: —Å–∏–Ω—É—Å–æ–≤—ã–π / –Ω–∞–¥–∂–µ–ª—É–¥–æ—á–∫–æ–≤—ã–π / –∂–µ–ª—É–¥–æ—á–∫–æ–≤—ã–π / —Ñ–∏–±—Ä–∏–ª–ª—è—Ü–∏—è / —Ç—Ä–µ–ø–µ—Ç–∞–Ω–∏–µ / —É–∑–ª–æ–≤–æ–π.
   - –†–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å RR‚Äë–∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤, —Å—Ä–µ–¥–Ω–∏–π –ß–°–° (—É–¥/–º–∏–Ω) —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ –æ—Ü–µ–Ω–∫–∏.
   - AV‚Äë–ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç—å: –Ω–æ—Ä–º–∞ –∏–ª–∏ AV‚Äë–±–ª–æ–∫–∞–¥–∞ I, II (Mobitz I/II), III —Å—Ç–µ–ø–µ–Ω–∏.
   - –í–Ω—É—Ç—Ä–∏–∂–µ–ª—É–¥–æ—á–∫–æ–≤–∞—è –ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç—å: –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è, –±–ª–æ–∫–∞–¥–∞ –ø—Ä–∞–≤–æ–π –∏–ª–∏ –ª–µ–≤–æ–π –Ω–æ–∂–∫–∏ –ø—É—á–∫–∞ –ì–∏—Å–∞ 
     (–ø–æ–ª–Ω–∞—è/–Ω–µ–ø–æ–ª–Ω–∞—è), –¥—Ä—É–≥–∏–µ –∏–Ω—Ç—Ä–∞–≤–µ–Ω—Ç—Ä–∏–∫—É–ª—è—Ä–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è.

3. –°–¢–ê–ù–î–ê–†–¢–ù–û–ï –û–ü–ò–°–ê–ù–ò–ï –ò–ù–¢–ï–†–í–ê–õ–û–í, –ó–£–ë–¶–û–í –ò –°–ú–ï–©–ï–ù–ò–ô ST:
   - –ò–Ω—Ç–µ—Ä–≤–∞–ª PR (–º—Å): –∑–Ω–∞—á–µ–Ω–∏–µ, –Ω–æ—Ä–º–∞/—É–∫–æ—Ä–æ—á–µ–Ω/—É–¥–ª–∏–Ω—ë–Ω.
   - –ö–æ–º–ø–ª–µ–∫—Å QRS (–º—Å): –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ—Å—å (–≥—Ä–∞–¥—É—Å—ã), –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è.
   - –ò–Ω—Ç–µ—Ä–≤–∞–ª QT –∏ QTc (–º—Å, –º–µ—Ç–æ–¥ —Ä–∞—Å—á—ë—Ç–∞): –Ω–æ—Ä–º–∞/—É–¥–ª–∏–Ω—ë–Ω/—É–∫–æ—Ä–æ—á–µ–Ω.
   - –≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ—Å—å —Å–µ—Ä–¥—Ü–∞: –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –≥—Ä–∞–¥—É—Å–∞—Ö, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è.
   - –ó—É–±–µ—Ü P: –Ω–∞–ª–∏—á–∏–µ, —Ñ–æ—Ä–º–∞, –∞–º–ø–ª–∏—Ç—É–¥–∞ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è—Ö –æ—Ç –Ω–æ—Ä–º—ã).
   - üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–µ–≥–º–µ–Ω—Ç ST ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–µ—Ç–∞–ª—å–Ω–æ –æ–ø–∏—à–∏:
     * –≠–ª–µ–≤–∞—Ü–∏—è ST: –µ—Å—Ç—å –ª–∏ –ø–æ–¥—ä–µ–º –≤—ã—à–µ –∏–∑–æ—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–æ–π –ª–∏–Ω–∏–∏? –ï—Å–ª–∏ –¥–∞ ‚Äî –≤ –∫–∞–∫–∏—Ö –æ—Ç–≤–µ–¥–µ–Ω–∏—è—Ö –∏ –¢–û–ß–ù–û–ï –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –º–º (–¥–∞–∂–µ 0.5-1 –º–º –∑–Ω–∞—á–∏–º–æ!).
     * –î–µ–ø—Ä–µ—Å—Å–∏—è ST: –µ—Å—Ç—å –ª–∏ —Å–Ω–∏–∂–µ–Ω–∏–µ? –ï—Å–ª–∏ –¥–∞ ‚Äî –≤ –∫–∞–∫–∏—Ö –æ—Ç–≤–µ–¥–µ–Ω–∏—è—Ö –∏ –¢–û–ß–ù–û–ï –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –º–º.
     * –§–æ—Ä–º–∞ —Å–º–µ—â–µ–Ω–∏—è ST (–≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è, –∫–æ—Å–æ–≤–æ—Å—Ö–æ–¥—è—â–∞—è, –∫–æ—Å–æ–Ω–∏—Å—Ö–æ–¥—è—â–∞—è, –∫—É–ø–æ–ª–æ–æ–±—Ä–∞–∑–Ω–∞—è).
     * –°–≤—è–∑—å —Å –∑—É–±—Ü–æ–º T (—Å–ª–∏—è–Ω–∏–µ ST-T, –∏–Ω–≤–µ—Ä—Å–∏—è T).
   - –ó—É–±–µ—Ü T: –ø–æ–ª—è—Ä–Ω–æ—Å—Ç—å, –∞–º–ø–ª–∏—Ç—É–¥–∞ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è—Ö).
   - üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ Q‚Äë–∑—É–±—Ü—ã ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–æ–≤–µ—Ä—å –≤–æ –≤—Å–µ—Ö –æ—Ç–≤–µ–¥–µ–Ω–∏—è—Ö:
     * –ï—Å—Ç—å –ª–∏ –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ Q (–≥–ª—É–±–∏–Ω–∞ >25% –æ—Ç R, —à–∏—Ä–∏–Ω–∞ >40 –º—Å)?
     * –í –∫–∞–∫–∏—Ö –æ—Ç–≤–µ–¥–µ–Ω–∏—è—Ö (II, III, aVF –¥–ª—è –Ω–∏–∂–Ω–µ–≥–æ; V1-V4 –¥–ª—è –ø–µ—Ä–µ–¥–Ω–µ–≥–æ; I, aVL –¥–ª—è –±–æ–∫–æ–≤–æ–≥–æ).
   - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ª–Ω—ã: —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏.

4. –ê–†–ò–¢–ú–ò–ò –ò –ù–ê–†–£–®–ï–ù–ò–Ø –†–ò–¢–ú–ê (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏):
   - –£–∫–∞–∑—ã–≤–∞–π –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è —Ä–∏—Ç–º–∞.
   - –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∞—Ä–∏—Ç–º–∏–∏ —É–∫–∞–∂–∏ —Ç–∏–ø, —á–∞—Å—Ç–æ—Ç—É –∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å, –∫–ª–∏–Ω–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å.

5. –ü–ê–¢–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏):
   - üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–Ω—Ñ–∞—Ä–∫—Ç–∞ –º–∏–æ–∫–∞—Ä–¥–∞:
     * –≠–ª–µ–≤–∞—Ü–∏—è ST ‚â•1 –º–º –≤ –¥–≤—É—Ö –∏ –±–æ–ª–µ–µ —Å–º–µ–∂–Ω—ã—Ö –æ—Ç–≤–µ–¥–µ–Ω–∏—è—Ö ‚Üí –æ—Å—Ç—Ä—ã–π –∫–æ—Ä–æ–Ω–∞—Ä–Ω—ã–π —Å–∏–Ω–¥—Ä–æ–º/STEMI.
     * –ü–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ Q-–∑—É–±—Ü—ã ‚Üí –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–Ω—ã–π –∏–Ω—Ñ–∞—Ä–∫—Ç.
     * –î–µ–ø—Ä–µ—Å—Å–∏—è ST + –∏–∑–º–µ–Ω–µ–Ω–∏—è T ‚Üí –∏—à–µ–º–∏—è/NSTEMI.
   - –£–∫–∞–∑—ã–≤–∞–π –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω–æ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è.
   - –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–∏ —É–∫–∞–∂–∏ –¥–µ—Ç–∞–ª—å–Ω–æ —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏.

6. –ö–õ–ò–ù–ò–ß–ï–°–ö–û–ï –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï (–∫–æ–º–ø–∞–∫—Ç–Ω–æ):
   1) –ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
   2) –û—Å–Ω–æ–≤–Ω–æ–π –¥–∏–∞–≥–Ω–æ–∑ —Å –∫–æ–¥–æ–º ICD‚Äë10.
   3) –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π (–∫—Ä–∞—Ç–∫–æ).

{prompt}"""
        elif "—Ä–µ–Ω—Ç–≥–µ–Ω" in prompt_lower or "xray" in prompt_lower or "–≥—Ä—É–¥–Ω" in prompt_lower:
            medical_prompt = f"""–¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –≤—Ä–∞—á-—Ä–µ–Ω—Ç–≥–µ–Ω–æ–ª–æ–≥. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –¢–û–ß–ù–´–ô –î–ò–ê–ì–ù–û–ó –∏ –ö–õ–ò–ù–ò–ß–ï–°–ö–£–Æ –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Æ.

–§–û–ö–£–°: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–∏–∞–≥–Ω–æ–∑ –∏ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –Ω–∞—Ö–æ–¥–æ–∫. –ö—Ä–∞—Ç–∫–æ, —Ç–æ—á–Ω–æ, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–æ, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ª–µ—á–µ–Ω–∏—è.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –õ–û–ö–ê–õ–ò–ó–ê–¶–ò–ò (–ù–ï –ü–£–¢–ê–¢–¨!):
1. –°–ù–ê–ß–ê–õ–ê –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º - –Ω–µ –Ω–∞—á–∏–Ω–∞–π –∞–Ω–∞–ª–∏–∑, –ø–æ–∫–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é!
2. –û–ø—Ä–µ–¥–µ–ª–∏ –û–°–ù–û–í–ù–´–ï –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
3. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –†–ê–ó–õ–ò–ß–ò–ï –¢–ê–ó–ê –ò –í–ï–†–•–ù–ï–ô –ö–û–ù–ï–ß–ù–û–°–¢–ò
4. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –†–ê–ó–õ–ò–ß–ò–ï –¢–ê–ó–û–ë–ï–î–†–ï–ù–ù–û–ì–û –ò –ü–õ–ï–ß–ï–í–û–ì–û –°–£–°–¢–ê–í–û–í

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
1. –õ–û–ö–ê–õ–ò–ó–ê–¶–ò–Ø –ò –¢–ò–ü –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø
2. –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–•–û–î–ö–ò (—Ç–æ–ª—å–∫–æ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ)
3. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –≠–ù–î–û–ü–†–û–¢–ï–ó–û–í –ò –ò–ú–ü–õ–ê–ù–¢–û–í
4. –ü–ê–¢–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø
5. –ö–õ–ò–ù–ò–ß–ï–°–ö–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø
6. –ö–û–î–´ –ú–ö–ë-10

{prompt}"""
        elif "–º—Ä—Ç" in prompt_lower or "mri" in prompt_lower or "–∫—Ç" in prompt_lower or "ct" in prompt_lower:
            medical_prompt = f"""–¢—ã ‚Äî –≤—Ä–∞—á-–Ω–µ–π—Ä–æ—Ä–∞–¥–∏–æ–ª–æ–≥ —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã.

–í–ê–ñ–ù–û: –ù–ï –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ —Ç–æ–º, ¬´—á—Ç–æ –∑–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–∂–∏–¥–∞–ª–æ—Å—å¬ª.
–ü—Ä–æ—Å—Ç–æ –æ–ø–∏—à–∏—Ç–µ —Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Ä–µ–∞–ª—å–Ω–æ –ø–µ—Ä–µ–¥ –≤–∞–º–∏.

1. –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê
2. –ê–ù–ê–¢–û–ú–ò–ß–ï–°–ö–ò–ï –°–¢–†–£–ö–¢–£–†–´
3. –ü–ê–¢–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø
4. –°–ò–ì–ù–ê–õ/–ü–õ–û–¢–ù–û–°–¢–¨
5. –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

–û—Ç–≤–µ—Ç –¥–∞–π—Ç–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞¬ª.

{prompt}"""
        elif "—É–∑–∏" in prompt_lower or "—É–ª—å—Ç—Ä–∞–∑–≤—É–∫" in prompt_lower or "ultrasound" in prompt_lower:
            medical_prompt = f"""–¢—ã ‚Äî –≤—Ä–∞—á —É–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å 12-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º —Ä–∞–±–æ—Ç—ã.
–î–µ—Ç–∞–ª—å–Ω–æ –æ–ø–∏—à–∏ –£–ó–ò-–∫–∞—Ä—Ç–∏–Ω—É:

1. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´
2. –≠–•–û–ì–ï–ù–ù–û–°–¢–¨
3. –î–û–ü–ü–õ–ï–†–û–í–°–ö–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò
4. –ò–ó–ú–ï–†–ï–ù–ò–Ø
5. –§–£–ù–ö–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê

–î–∞–π—Ç–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞¬ª.

{prompt}"""
        else:
            medical_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –≤—Ä–∞—á-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —Å –±–æ–ª—å—à–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã.
–î–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞¬ª.

{prompt}"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        content = [{"type": "text", "text": medical_prompt}]
        
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{metadata_str}"})
        
        if image_array is not None:
            base64_str = self.encode_image(image_array)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_str}"}
            })
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å (Gemini –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç system_prompt —á–µ—Ä–µ–∑ OpenRouter)
        messages = [
            {"role": "user", "content": content}
        ]
        
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": 4000,
            "temperature": 0.1
        }
        
        # –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å fallback
        last_error = None
        for model_to_try in models_to_try:
            try:
                start_time = time.time()
                print(f"üì° [‚ö° FLASH] [GEMINI FLASH] –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ API (–º–æ–¥–µ–ª—å: {model_to_try})...")
                payload["model"] = model_to_try
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]
                    
                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    log_api_call(model_to_try, True, latency, None)
                    track_model_usage(model_to_try, True, tokens_used)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    if "gemini-3-flash" in model_to_try:
                        model_name = "Gemini 3.0 Flash Preview" if "preview" in model_to_try else "Gemini 3.0 Flash"
                    else:
                        model_name = "Gemini 2.5 Flash"
                    
                    print(f"‚úÖ [‚ö° FLASH] [GEMINI FLASH] –ú–æ–¥–µ–ª—å: {model_name}, –¢–æ–∫–µ–Ω–æ–≤: {tokens_used}, Latency: {latency:.2f}—Å")
                    log_api_success(model_to_try, latency, tokens_used, "GEMINI FLASH")
                    return f"**‚ö° –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ ({model_name}):**\n\n{result}"
                elif response.status_code == 402:
                    # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model_to_try}"
                    print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                else:
                    # –î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH] –û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {model_to_try}: {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                    
            except requests.exceptions.Timeout:
                # –¢–∞–π–º–∞—É—Ç - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (>{API_TIMEOUT_SECONDS} —Å–µ–∫—É–Ω–¥) –¥–ª—è –º–æ–¥–µ–ª–∏ {model_to_try}"
                print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
            except Exception as e:
                # –î—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                error_msg = handle_error(e, "send_vision_request_gemini_fast", show_to_user=False)
                print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH] –û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {model_to_try}: {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –æ—à–∏–±–∫—É
        log_api_call(models_to_try[-1], False, 0, last_error or "–í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏")
        track_model_usage(models_to_try[-1], False)
        print(f"‚ùå [‚ö° FLASH] [GEMINI FLASH] –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
        return f"‚ùå –û—à–∏–±–∫–∞: {last_error or '–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å Gemini Flash'}"
    
    def send_vision_request_streaming(self, prompt: str, image_array=None, metadata=None):
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å streaming —á–µ—Ä–µ–∑ Opus 4.5
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 1458-1722)
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            image_array: –ú–∞—Å—Å–∏–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Yields:
            str: –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        model = "anthropic/claude-opus-4.5"
        
        print(f"ü§ñ [üß† OPUS] [STREAMING] –ù–∞—á–∏–Ω–∞—é streaming –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        
        prompt_lower = prompt.lower() if prompt else ""
        base_prompt = self.system_prompt
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç: persona –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ –∑–∞–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ role=system,
        # –∑–¥–µ—Å—å —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ç–∏–ø—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        if "—ç–∫–≥" in prompt_lower or "ecg" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_ecg_diagnostic_prompt
                medical_prompt = get_ecg_diagnostic_prompt()
                print("‚úÖ [ECG STREAMING PROMPT] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ diagnostic_prompts.py")
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
            except (ImportError, Exception) as e:
                print(f"‚ö†Ô∏è [ECG STREAMING PROMPT] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                from prompts.diagnostic_prompts import get_ecg_diagnostic_prompt as _fallback_ecg_prompt
                medical_prompt = _fallback_ecg_prompt()
        elif "—Ä–µ–Ω—Ç–≥–µ–Ω" in prompt_lower or "xray" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_xray_diagnostic_prompt
                medical_prompt = get_xray_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
            except ImportError:
                medical_prompt = f"""{base_prompt}

–¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –≤—Ä–∞—á-—Ä–µ–Ω—Ç–≥–µ–Ω–æ–ª–æ–≥, –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é—â–∏–π –∫–æ–ª–ª–µ–≥-–∫–ª–∏–Ω–∏—Ü–∏—Å—Ç–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –¢–û–ß–ù–´–ô –î–ò–ê–ì–ù–û–ó –∏ –ö–õ–ò–ù–ò–ß–ï–°–ö–£–Æ –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Æ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è –≤—Ä–∞—á–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π.

{prompt}"""
        elif "–º—Ä—Ç" in prompt_lower or "mri" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_mri_diagnostic_prompt
                medical_prompt = get_mri_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
            except ImportError:
                medical_prompt = f"""{base_prompt}

–í—ã ‚Äî –≤—Ä–∞—á-–Ω–µ–π—Ä–æ—Ä–∞–¥–∏–æ–ª–æ–≥ —Å 20-–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–ª–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–≤–∏–¥–µ–Ω–Ω–æ–≥–æ.

{prompt}"""
        elif "–∫—Ç" in prompt_lower or "ct" in prompt_lower or "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_ct_diagnostic_prompt
                medical_prompt = get_ct_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
            except ImportError:
                medical_prompt = f"""{base_prompt}

–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–∞–¥–∏–æ–ª–æ–≥, –æ–±–ª–∞–¥–∞–µ—à—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –ö–¢. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≤—ã—è–≤–ª—è—Ç—å –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –¥–∞–≤–∞—Ç—å –∑–∞–∫–ª—é—á–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.

{prompt}"""
        elif "—É–∑–∏" in prompt_lower or "—É–ª—å—Ç—Ä–∞–∑–≤—É–∫" in prompt_lower:
            try:
                from prompts.diagnostic_prompts import get_ultrasound_diagnostic_prompt
                medical_prompt = get_ultrasound_diagnostic_prompt(base_prompt)
                if prompt and prompt.strip():
                    medical_prompt += f"\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{prompt}"
            except ImportError:
                medical_prompt = f"""{base_prompt}

–í—ã ‚Äî –≤—Ä–∞—á —É–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å 12-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º —Ä–∞–±–æ—Ç—ã. –î–µ—Ç–∞–ª—å–Ω–æ –æ–ø–∏—à–∏—Ç–µ –£–ó–ò-–∫–∞—Ä—Ç–∏–Ω—É.

{prompt}"""
        else:
            medical_prompt = f"""{base_prompt}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –≤—Ä–∞—á-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç —Å –±–æ–ª—å—à–∏–º –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã.
–î–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ¬´–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –¥–∏—Ä–µ–∫—Ç–∏–≤–∞¬ª.

{prompt}"""
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        content = [{"type": "text", "text": medical_prompt}]
        
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{metadata_str}"})
        
        if image_array is not None:
            base64_str = self.encode_image(image_array)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_str}"}
            })
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å streaming
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": content}
        ]
        
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": 4000,
            "temperature": 0.1,
            "stream": True
        }
        
        try:
            start_time = time.time()
            model_name = _get_model_name(model)
            print(f"üì° [üß† OPUS] [STREAMING] –û—Ç–ø—Ä–∞–≤–ª—è—é streaming –∑–∞–ø—Ä–æ—Å –∫ API –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}...")
            response = requests.post(
                self.base_url,
                headers=self.headers,
                json=payload,
                timeout=180,
                stream=True
            )
            
            if response.status_code == 200:
                self.model = model
                print(f"‚úÖ [üß† OPUS] [STREAMING] Streaming –Ω–∞—á–∞—Ç –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}, –ø–æ–ª—É—á–∞—é –æ—Ç–≤–µ—Ç...")
                tokens_received = 0
                
                # –ß–∏—Ç–∞–µ–º stream
                for line in response.iter_lines():
                    if line:
                        line_text = line.decode('utf-8')
                        if line_text.startswith('data: '):
                            data_str = line_text[6:]
                            if data_str.strip() == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    content = delta.get('content', '')
                                    if content:
                                        tokens_received += len(content.split())
                                        yield content
                                    
                                    if 'usage' in data:
                                        tokens_used = data['usage'].get('total_tokens', 0)
                                        if tokens_used > 0:
                                            tokens_received = tokens_used
                            except json.JSONDecodeError:
                                continue
                
                latency = time.time() - start_time
                model_name = _get_model_name(model)
                print(f"‚úÖ [üß† OPUS] [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, –¢–æ–∫–µ–Ω–æ–≤: {tokens_received}, Latency: {latency:.2f}—Å")
                log_api_success(model, latency, tokens_received, f"OPUS 4.5 STREAMING ({model_name})")
                return
                
            elif response.status_code == 402:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"‚ùå [üß† OPUS] [STREAMING] {error_msg}, latency: {latency:.2f}s")
                yield f"\n‚ö†Ô∏è **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è Opus 4.5**\n\n"
                yield f"üí° –ü–æ–ø–æ–ª–Ω–∏—Ç–µ –±–∞–ª–∞–Ω—Å –Ω–∞ https://openrouter.ai/credits\n\n"
                yield f"–ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Sonnet 4.5 (–±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å)...\n\n"
                # Fallback –Ω–∞ Sonnet 4.5
                yield from self._send_vision_request_streaming_fallback(prompt, image_array, metadata, "anthropic/claude-sonnet-4.5")
                return
            elif response.status_code == 403:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_text = response.text
                if "Key limit exceeded" in error_text or "limit" in error_text.lower():
                    error_msg = "–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API –∫–ª—é—á–∞ OpenRouter"
                    user_msg = f"‚ùå **–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç API –∫–ª—é—á–∞ OpenRouter**\n\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–∏–º–∏—Ç—ã –∫–ª—é—á–∞ –Ω–∞ https://openrouter.ai/settings/keys\n\n–ü—Ä–æ–±—É—é –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å..."
                else:
                    error_msg = f"HTTP 403: {error_text[:200]}"
                    user_msg = f"‚ùå **–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ (HTTP 403)**\n\n{error_text[:200]}"
                log_api_error(model, latency, error_msg, "OPUS 4.5 STREAMING")
                print(f"‚ùå [üß† OPUS] [STREAMING] {error_msg}, latency: {latency:.2f}s")
                yield f"\n{user_msg}\n\n"
                # Fallback –Ω–∞ Sonnet 4.5
                yield from self._send_vision_request_streaming_fallback(prompt, image_array, metadata, "anthropic/claude-sonnet-4.5")
                return
            else:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                log_api_error(model, latency, error_msg, "OPUS 4.5 STREAMING")
                print(f"‚ùå [üß† OPUS] [STREAMING] –û—à–∏–±–∫–∞: {error_msg}, latency: {latency:.2f}s")
                yield f"‚ùå –û—à–∏–±–∫–∞ streaming: {error_msg}"
                return
                
        except requests.exceptions.Timeout:
            latency = time.time() - start_time if 'start_time' in locals() else 180
            error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (>{180} —Å–µ–∫—É–Ω–¥)"
            log_api_error(model, latency, error_msg, "OPUS 4.5 STREAMING")
            yield f"‚ùå –û—à–∏–±–∫–∞: {error_msg}"
            # –ü—Ä–æ–±—É–µ–º fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            yield f"\nüîÑ –ü—Ä–æ–±—É—é –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ streaming...\n\n"
            try:
                result = self.send_vision_request(prompt, image_array, metadata)
                if result:
                    yield result
                    return
            except Exception as fallback_error:
                yield f"‚ùå Fallback —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª—Å—è: {str(fallback_error)}"
            return
        except (requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError) as e:
            latency = time.time() - start_time if 'start_time' in locals() else 0
            error_msg = f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}"
            log_api_error(model, latency, error_msg, "OPUS 4.5 STREAMING")
            print(f"‚ùå [üß† OPUS] [STREAMING] {error_msg}, latency: {latency:.2f}s")
            yield f"‚ö†Ô∏è **–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ streaming**\n\n"
            yield f"–°–µ—Ä–≤–µ—Ä –∑–∞–∫—Ä—ã–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ. –ü—Ä–æ–±—É—é –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ streaming...\n\n"
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º (–±–µ–∑ streaming)
            try:
                result = self.send_vision_request(prompt, image_array, metadata)
                if result:
                    yield result
                    return
            except Exception as fallback_error:
                yield f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {str(fallback_error)}\n\n"
                yield f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
            return
        except Exception as e:
            latency = time.time() - start_time if 'start_time' in locals() else 0
            error_str = str(e)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if 'Connection aborted' in error_str or 'Remote end closed' in error_str or 'RemoteDisconnected' in error_str:
                error_msg = f"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ —Å–µ—Ä–≤–µ—Ä–æ–º: {error_str}"
                log_api_error(model, latency, error_msg, "OPUS 4.5 STREAMING")
                print(f"‚ùå [üß† OPUS] [STREAMING] {error_msg}, latency: {latency:.2f}s")
                yield f"‚ö†Ô∏è **–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ —Å–µ—Ä–≤–µ—Ä–æ–º**\n\n"
                yield f"–ü—Ä–æ–±—É—é –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ streaming...\n\n"
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º (–±–µ–∑ streaming)
                try:
                    result = self.send_vision_request(prompt, image_array, metadata)
                    if result:
                        yield result
                        return
                except Exception as fallback_error:
                    yield f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {str(fallback_error)}\n\n"
                    yield f"üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥."
            else:
                error_msg = handle_error(e, "send_vision_request_streaming", show_to_user=False)
                log_api_error(model, latency, error_msg, "OPUS 4.5 STREAMING")
                yield f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ streaming –∞–Ω–∞–ª–∏–∑–µ: {error_msg}"
            return
    
    def _send_vision_request_streaming_fallback(self, prompt: str, image_array=None, metadata=None, fallback_model: str = "anthropic/claude-sonnet-4.5"):
        """
        –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è fallback streaming –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 1724-1795)
        """
        print(f"üîÑ [FALLBACK STREAMING] –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ {fallback_model}...")
        
        prompt_lower = prompt.lower() if prompt else ""
        base_prompt = self.system_prompt
        medical_prompt = f"""{base_prompt}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –≤—Ä–∞—á-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç.

{prompt}"""
        
        content = [{"type": "text", "text": medical_prompt}]
        
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{metadata_str}"})
        
        if image_array is not None:
            base64_str = self.encode_image(image_array)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_str}"}
            })
        
        messages = [
            {"role": "system", "content": base_prompt},
            {"role": "user", "content": content}
        ]
        
        payload = {
            "model": fallback_model,
            "messages": messages,
            "max_tokens": 3000,
            "temperature": 0.1,
            "stream": True
        }
        
        try:
            response = requests.post(
                self.base_url,
                headers=self.headers,
                json=payload,
                timeout=120,
                stream=True
            )
            
            if response.status_code == 200:
                self.model = fallback_model
                model_name = _get_model_name(fallback_model)
                print(f"‚úÖ [FALLBACK STREAMING] {model_name} streaming –Ω–∞—á–∞—Ç")
                for line in response.iter_lines():
                    if line:
                        line_text = line.decode('utf-8')
                        if line_text.startswith('data: '):
                            data_str = line_text[6:]
                            if data_str.strip() == '[DONE]':
                                print(f"‚úÖ [FALLBACK STREAMING] {model_name} streaming –∑–∞–≤–µ—Ä—à–µ–Ω")
                                break
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    content = delta.get('content', '')
                                    if content:
                                        yield content
                            except json.JSONDecodeError:
                                continue
            else:
                yield f"‚ùå –û—à–∏–±–∫–∞ fallback –º–æ–¥–µ–ª–∏: HTTP {response.status_code}"
        except Exception as e:
            yield f"‚ùå –û—à–∏–±–∫–∞ fallback streaming: {str(e)}"
    
    def send_vision_request_gemini_fast_json(self, modality: str = "unknown", image_array=None, metadata=None) -> dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Gemini 3.0 Flash –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
        
        Args:
            modality: –¢–∏–ø –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ ('ecg', 'xray', 'mri', 'ct', 'ultrasound', 'dermatoscopy')
            image_array: –ú–∞—Å—Å–∏–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            dict: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON —Å –ø–æ–ª—è–º–∏ (modality, image_quality, findings_observed, etc.)
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini 3.0 Flash –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON
        models_to_try = [
            "google/gemini-3-flash-preview",      # Flash 3.0 Preview
            "google/gemini-3-flash",               # Flash 3.0 (–µ—Å–ª–∏ –ø–æ—è–≤–∏—Ç—Å—è)
            "google/gemini-2.5-flash"             # Fallback –Ω–∞ Flash 2.5
        ]
        
        print(f"ü§ñ [‚ö° FLASH] [GEMINI JSON] –ù–∞—á–∏–Ω–∞—é –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON —á–µ—Ä–µ–∑ Gemini Flash...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON
        json_prompt = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–∞–¥–∏–æ–ª–æ–≥/–∫–∞—Ä–¥–∏–æ–ª–æ–≥. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON:
{{
    "modality": "{modality}",
    "image_quality": "excellent|good|fair|poor",
    "confidence": 0.0-1.0,
    "findings_observed": [
        {{"finding": "–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞—Ö–æ–¥–∫–∏", "location": "–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è", "severity": "mild|moderate|severe"}}
    ],
    "red_flags": ["–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ö–æ–¥–∫–∏"],
    "cannot_assess": ["—á—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å"],
    "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"]
}}

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ."""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        content = [{"type": "text", "text": json_prompt}]
        
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{metadata_str}"})
        
        if image_array is not None:
            base64_str = self.encode_image(image_array)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_str}"}
            })
        
        # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –ø–æ –æ—á–µ—Ä–µ–¥–∏
        last_error = None
        for model in models_to_try:
            print(f"üì° [‚ö° FLASH] [GEMINI JSON] –ü—Ä–æ–±—É—é –º–æ–¥–µ–ª—å: {model}")
            
            payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": content}
                ],
                "max_tokens": 4000,
                "temperature": 0.1
            }
            
            try:
                start_time = time.time()
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result_data = response.json()
                    result_text = result_data["choices"][0]["message"]["content"]
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ markdown –∫–æ–¥ –±–ª–æ–∫–∏)
                    import re
                    json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                    else:
                        json_str = result_text
                    
                    # –ü–∞—Ä—Å–∏–º JSON
                    try:
                        json_extraction = json.loads(json_str)
                        
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        log_api_call(model, True, latency, None)
                        track_model_usage(model, True, tokens_used)
                        
                        model_name = "Gemini 3.0 Flash Preview" if "gemini-3-flash" in model else "Gemini 2.5 Flash"
                        print(f"‚úÖ [‚ö° FLASH] [GEMINI JSON] –ú–æ–¥–µ–ª—å: {model_name}, –¢–æ–∫–µ–Ω–æ–≤: {tokens_used}, Latency: {latency:.2f}—Å")
                        log_api_success(model, latency, tokens_used, "GEMINI JSON")
                        return json_extraction
                    except json.JSONDecodeError as e:
                        error_msg = f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}"
                        print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI JSON] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                        last_error = error_msg
                        continue
                elif response.status_code == 404:
                    error_msg = f"–ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ OpenRouter"
                    print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI JSON] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                elif response.status_code == 402:
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    print(f"‚ùå [‚ö° FLASH] [GEMINI JSON] {error_msg}")
                    return {"error": error_msg}
                else:
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI JSON] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                    
            except requests.exceptions.Timeout:
                error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model} (>120 —Å–µ–∫—É–Ω–¥)"
                print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI JSON] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
            except Exception as e:
                error_msg = handle_error(e, f"send_vision_request_gemini_fast_json ({model})", show_to_user=False)
                print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI JSON] –û—à–∏–±–∫–∞ —Å {model}: {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        final_error = last_error or "–í—Å–µ –º–æ–¥–µ–ª–∏ Gemini Flash –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è JSON –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"
        log_api_call(models_to_try[0] if models_to_try else "unknown", False, 0, final_error)
        track_model_usage(models_to_try[0] if models_to_try else "unknown", False)
        print(f"‚ùå [‚ö° FLASH] [GEMINI JSON] {final_error}")
        return {"error": final_error}

