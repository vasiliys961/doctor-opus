"""
Text –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π
–°–û–î–ï–†–ñ–ò–¢ –í–°–Æ –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–£–Æ –õ–û–ì–ò–ö–£ –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô!

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–æ–¥—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π:
- get_response() - –æ–±—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- get_response_streaming() - streaming –∑–∞–ø—Ä–æ—Å—ã
- get_response_without_system() - –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
- general_medical_consultation() - –æ–±—â–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏
- analyze_ecg_data() - –∞–Ω–∞–ª–∏–∑ –≠–ö–ì –¥–∞–Ω–Ω—ã—Ö
"""

import time
import requests
import json
import sys
from typing import Optional, Generator

from .base_client import BaseAPIClient
from .diagnostic_prompts import get_system_prompt
from .logging_handler import log_api_error, log_api_success, _get_model_name
from utils.error_handler import handle_error, log_api_call
from utils.performance_monitor import track_model_usage
from utils.cost_calculator import calculate_cost, format_cost_log


class TextClient(BaseAPIClient):
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í—Å—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π!
    –í—Å–µ –º–µ—Ç–æ–¥—ã —è–≤–ª—è—é—Ç—Å—è –¢–û–ß–ù–û–ô –ö–û–ü–ò–ï–ô –∏–∑ claude_assistant.py
    """
    
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1/chat/completions"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Text –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            api_key: API –∫–ª—é—á OpenRouter
            base_url: –ë–∞–∑–æ–≤—ã–π URL API
        """
        super().__init__(api_key, base_url)
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
        self.system_prompt = get_system_prompt()
        
        # –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: Claude 4.5 —Å–µ—Ä–∏—è + Llama
        self.models = [
            "anthropic/claude-opus-4.5",
            "anthropic/claude-sonnet-4.5",
            "anthropic/claude-haiku-4.5",
            "meta-llama/llama-3.2-90b-vision-instruct"
        ]
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º Opus
        self.model = self.models[0]
    
    def get_response(
        self,
        user_message: str,
        context: str = "",
        use_sonnet_4_5: bool = False
    ) -> str:
        """
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª—É—á—à–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –ª–æ–≥–∏–∫–∏ –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 1829-1900)
        
        Args:
            user_message: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            use_sonnet_4_5: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Sonnet 4.5 (–¥–ª—è –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤)
        
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        """
        full_message = f"{context}\n\n–í–æ–ø—Ä–æ—Å: {user_message}" if context else user_message
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –º–æ–¥–µ–ª—å Sonnet 4.5 –¥–ª—è –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, —Å—Ç–∞–≤–∏–º –µ—ë –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        if use_sonnet_4_5:
            models_to_try = ["anthropic/claude-sonnet-4.5"] + [m for m in self.models if m != "anthropic/claude-sonnet-4.5"]
        else:
            models_to_try = self.models
        
        # –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É
        for model in models_to_try:
            try:
                start_time = time.time()
                model_name = _get_model_name(model)
                print(f"ü§ñ [{model_name}] –ù–∞—á–∏–Ω–∞—é —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å...")
                
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": full_message}
                    ],
                    "max_tokens": 8000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    "temperature": 0.2
                }
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                prompt_size = len(full_message)
                if prompt_size > 50000:
                    print(f"‚ö†Ô∏è [{model_name}] –ë–æ–ª—å—à–æ–π –ø—Ä–æ–º–ø—Ç: {prompt_size} —Å–∏–º–≤–æ–ª–æ–≤. –ú–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏.")
                
                print(f"üì° [{model_name}] –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ API...")
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=300)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]

                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    input_tokens = result_data.get("usage", {}).get("prompt_tokens", tokens_used // 2)
                    output_tokens = result_data.get("usage", {}).get("completion_tokens", tokens_used // 2)
                    
                    # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω—ã –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏—Ö –∏–∑ –æ—Ç–≤–µ—Ç–∞
                    if input_tokens == tokens_used // 2 and output_tokens == tokens_used // 2:
                        input_tokens = result_data.get("usage", {}).get("prompt_tokens", 0)
                        output_tokens = result_data.get("usage", {}).get("completion_tokens", 0)
                        if input_tokens == 0 and output_tokens == 0:
                            input_tokens = tokens_used // 2
                            output_tokens = tokens_used // 2
                    
                    log_api_call(model, True, latency, None)
                    track_model_usage(model, True, tokens_used)
                    
                    # –†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏
                    cost_info = calculate_cost(input_tokens, output_tokens, model)
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                    print(f"‚úÖ [{model_name}] –ó–∞–ø—Ä–æ—Å –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {latency:.2f}—Å")
                    print(f"   üìä {format_cost_log(model, input_tokens, output_tokens, tokens_used)}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    try:
                        import streamlit as st
                        st.session_state.last_request_info = {
                            'model': model,
                            'tokens': tokens_used,
                            'input_tokens': input_tokens,
                            'output_tokens': output_tokens,
                            'latency': latency,
                            'cost_usd': cost_info['total_cost_usd'],
                            'cost_units': cost_info['total_cost_units']
                        }
                    except:
                        pass

                    self.model = model
                    return result
                elif response.status_code == 402:
                    # –û—à–∏–±–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –∫—Ä–µ–¥–∏—Ç–æ–≤
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    print(f"‚ö†Ô∏è {error_msg}. –ü—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    continue
                else:
                    error_msg = f"HTTP {response.status_code}"
                    log_api_error(model, latency, error_msg)
                    continue
                    
            except requests.exceptions.Timeout:
                latency = time.time() - start_time if 'start_time' in locals() else 300
                error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (>{300} —Å–µ–∫—É–Ω–¥)"
                log_api_error(model, latency, error_msg)
                continue
            except Exception as e:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = handle_error(e, f"get_response ({model})", show_to_user=False)
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {model}: {e}")
                continue
        
        return "‚ùå –û—à–∏–±–∫–∞: –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ API –∫–ª—é—á–∏."
    
    def get_response_streaming(
        self,
        user_message: str,
        context: str = "",
        use_sonnet_4_5: bool = False,
        force_opus: bool = False
    ) -> Generator[str, None, None]:
        """
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å streaming - —Ç–µ–∫—Å—Ç –ø–æ—è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –ª–æ–≥–∏–∫–∏ –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 1902-2038)
        
        Args:
            user_message: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            use_sonnet_4_5: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Sonnet 4.5 (–¥–ª—è –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤)
            force_opus: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Opus 4.5
        
        Yields:
            str: –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        full_message = f"{context}\n\n–í–æ–ø—Ä–æ—Å: {user_message}" if context else user_message
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π Opus, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ–≥–æ
        if force_opus:
            models_to_try = ["anthropic/claude-opus-4.5"]
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –º–æ–¥–µ–ª—å Sonnet 4.5, —Å—Ç–∞–≤–∏–º –µ—ë –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        elif use_sonnet_4_5:
            models_to_try = ["anthropic/claude-sonnet-4.5"] + [m for m in self.models if m != "anthropic/claude-sonnet-4.5"]
        else:
            models_to_try = self.models
        
        # –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É
        start_time = time.time()
        for model in models_to_try:
            try:
                model_name = _get_model_name(model)
                model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                force_msg = " [FORCE_OPUS]" if force_opus else ""
                print(f"ü§ñ [{model_type}]{force_msg} [STREAMING] –ù–∞—á–∏–Ω–∞—é streaming —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}...")
                
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": full_message}
                    ],
                    "max_tokens": 8000,
                    "temperature": 0.2,
                    "stream": True
                }
                
                force_msg = " [FORCE_OPUS]" if force_opus else ""
                print(f"üì° [{model_type}]{force_msg} [STREAMING] –û—Ç–ø—Ä–∞–≤–ª—è—é streaming –∑–∞–ø—Ä–æ—Å –∫ API –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}...")
                response = requests.post(
                    self.base_url,
                    headers=self.headers,
                    json=payload,
                    timeout=180,
                    stream=True
                )
                
                if response.status_code == 200:
                    self.model = model
                    force_msg = " [FORCE_OPUS]" if force_opus else ""
                    print(f"‚úÖ [{model_type}]{force_msg} [STREAMING] Streaming –Ω–∞—á–∞—Ç –¥–ª—è –º–æ–¥–µ–ª–∏: {model_name}, –ø–æ–ª—É—á–∞—é –æ—Ç–≤–µ—Ç...", file=sys.stderr)
                    tokens_received = 0
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º iter_content —Å –±—É—Ñ–µ—Ä–æ–º, –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ (—Ç–∞–º —Ä–∞–±–æ—Ç–∞–ª–æ)
                    buffer = b''
                    try:
                        for chunk in response.iter_content(chunk_size=8192, decode_unicode=False):
                            if chunk:
                                buffer += chunk
                                
                                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
                                while b'\n' in buffer:
                                    line, buffer = buffer.split(b'\n', 1)
                                    if line:
                                        try:
                                            line_text = line.decode('utf-8')
                                        except UnicodeDecodeError:
                                            continue
                                        
                                        if line_text.startswith('data: '):
                                            data_str = line_text[6:]
                                            if data_str.strip() == '[DONE]':
                                                latency = time.time() - start_time
                                                model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                                                force_msg = " [FORCE_OPUS]" if force_opus else ""
                                                context_msg = f"STREAMING ({model_name})" + (force_msg if force_opus else "")
                                                
                                                # –î–ª—è streaming —Ç–æ–∫–µ–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–º–∏
                                                # –ò—Å–ø–æ–ª—å–∑—É–µ–º tokens_received –∫–∞–∫ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                                                input_tokens_stream = tokens_received // 2
                                                output_tokens_stream = tokens_received // 2
                                                cost_info = calculate_cost(input_tokens_stream, output_tokens_stream, model)
                                                
                                                print(f"‚úÖ [{model_type}]{force_msg} [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, –¢–æ–∫–µ–Ω–æ–≤: {tokens_received}, Latency: {latency:.2f}—Å", file=sys.stderr)
                                                print(f"   üìä {format_cost_log(model, input_tokens_stream, output_tokens_stream, tokens_received)}", file=sys.stderr)
                                                log_api_success(model, latency, tokens_received, context_msg)
                                                return
                                            try:
                                                data = json.loads(data_str)
                                                if 'choices' in data and len(data['choices']) > 0:
                                                    delta = data['choices'][0].get('delta', {})
                                                    content = delta.get('content', '')
                                                    if content:
                                                        tokens_received += len(content.split())
                                                        yield content
                                                    
                                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                                                    choice = data['choices'][0]
                                                    if choice.get('finish_reason'):
                                                        model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                                                        print(f"‚úÖ [{model_type}] [STREAMING] Streaming –∑–∞–≤–µ—Ä—à–µ–Ω (finish_reason: {choice['finish_reason']})", file=sys.stderr)
                                                        return
                                                    
                                                    if 'usage' in data:
                                                        tokens_used = data['usage'].get('total_tokens', 0)
                                                        if tokens_used > 0:
                                                            tokens_received = tokens_used
                                                            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞
                                                            input_tokens_stream = data['usage'].get('prompt_tokens', tokens_received // 2)
                                                            output_tokens_stream = data['usage'].get('completion_tokens', tokens_received // 2)
                                            except json.JSONDecodeError:
                                                continue
                    except (requests.exceptions.ChunkedEncodingError, requests.exceptions.ConnectionError) as e:
                        model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                        print(f"‚ö†Ô∏è [{model_type}] [STREAMING] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞: {e}", file=sys.stderr)
                        yield f"\n\n‚ö†Ô∏è **–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è streaming –ø–æ—Ç–æ–∫–∞**: {str(e)}\n\n"
                    
                    return
                elif response.status_code == 402:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    model_name = _get_model_name(model)
                    model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                    force_msg = " [FORCE_OPUS]" if force_opus else ""
                    print(f"‚ùå [{model_type}]{force_msg} [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, {error_msg}, Latency: {latency:.2f}—Å")
                    if force_opus:
                        yield f"\n‚ùå **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è Opus 4.5**\n\n"
                        yield f"üí° –ü–æ–ø–æ–ª–Ω–∏—Ç–µ –±–∞–ª–∞–Ω—Å –Ω–∞ https://openrouter.ai/credits\n\n"
                        return
                    if model == "anthropic/claude-sonnet-4.5":
                        yield f"\n‚ö†Ô∏è **Sonnet 4.5 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤). –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å...**\n\n"
                    else:
                        yield f"\n‚ö†Ô∏è **{model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤). –ü—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...**\n\n"
                    continue
                elif response.status_code == 403:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_text = response.text
                    model_name = _get_model_name(model)
                    model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                    force_msg = " [FORCE_OPUS]" if force_opus else ""
                    if "Key limit exceeded" in error_text or "limit" in error_text.lower():
                        error_msg = f"HTTP 403: –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç API –∫–ª—é—á–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                        user_msg = f"‚ùå **–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç API –∫–ª—é—á–∞ OpenRouter**\n\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–∏–º–∏—Ç—ã –Ω–∞ https://openrouter.ai/settings/keys"
                    else:
                        error_msg = f"HTTP 403: {error_text[:200]}"
                        user_msg = f"‚ùå **–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ (HTTP 403)**\n\n{error_text[:200]}"
                    log_api_error(model, latency, error_msg, f"STREAMING{force_msg}")
                    print(f"‚ùå [{model_type}]{force_msg} [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, {error_msg}, Latency: {latency:.2f}—Å")
                    if force_opus:
                        yield f"\n{user_msg}\n\n"
                        return
                    yield f"\n{user_msg}\n–ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å...\n\n"
                    continue
                else:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    model_name = _get_model_name(model)
                    model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                    force_msg = " [FORCE_OPUS]" if force_opus else ""
                    log_api_error(model, latency, error_msg, f"STREAMING{force_msg}")
                    print(f"‚ùå [{model_type}]{force_msg} [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, –û—à–∏–±–∫–∞: {error_msg}, Latency: {latency:.2f}—Å")
                    if force_opus:
                        yield f"‚ùå –û—à–∏–±–∫–∞: {error_msg}"
                        return
                    continue
                    
            except requests.exceptions.Timeout:
                latency = time.time() - start_time if 'start_time' in locals() else 300
                error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (>{180} —Å–µ–∫—É–Ω–¥)"
                log_api_error(model, latency, error_msg)
                if force_opus:
                    yield f"‚ùå –û—à–∏–±–∫–∞: {error_msg}"
                    return
                continue
            except (requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError) as e:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}"
                log_api_error(model, latency, error_msg)
                model_name = _get_model_name(model)
                model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                force_msg = " [FORCE_OPUS]" if force_opus else ""
                print(f"‚ùå [{model_type}]{force_msg} [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, {error_msg}, Latency: {latency:.2f}—Å")
                if force_opus:
                    yield f"‚ö†Ô∏è **–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ streaming**\n\n–°–µ—Ä–≤–µ—Ä –∑–∞–∫—Ä—ã–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å.\n\n"
                    return
                yield f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}. –ü—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é...\n\n"
                continue
            except Exception as e:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_str = str(e)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                if 'Connection aborted' in error_str or 'Remote end closed' in error_str or 'RemoteDisconnected' in error_str:
                    error_msg = f"–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ —Å–µ—Ä–≤–µ—Ä–æ–º: {error_str}"
                    log_api_error(model, latency, error_msg)
                    model_name = _get_model_name(model)
                    model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                    force_msg = " [FORCE_OPUS]" if force_opus else ""
                    print(f"‚ùå [{model_type}]{force_msg} [STREAMING] –ú–æ–¥–µ–ª—å: {model_name}, {error_msg}, Latency: {latency:.2f}—Å")
                    if force_opus:
                        yield f"‚ö†Ô∏è **–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ —Å–µ—Ä–≤–µ—Ä–æ–º**\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥.\n\n"
                        return
                    yield f"‚ö†Ô∏è –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}. –ü—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é...\n\n"
                else:
                    error_msg = handle_error(e, "get_response_streaming", show_to_user=False)
                    log_api_error(model, latency, error_msg)
                    if force_opus:
                        yield f"‚ùå –û—à–∏–±–∫–∞: {error_msg}"
                        return
                continue
        
        yield "‚ùå –û—à–∏–±–∫–∞: –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è streaming. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏ API –∫–ª—é—á–∏."
    
    def general_medical_consultation(self, user_question: str) -> str:
        """
        –û–±—â–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∞ 2404)
        
        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        """
        return self.get_response(user_question)
    
    def get_response_without_system(self, user_message: str, force_opus: bool = False) -> str:
        """
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ë–ï–ó –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 2040-2113)
        
        Args:
            user_message: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–Ω—É—Ç—Ä–∏)
            force_opus: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Opus 4.5
        
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç Opus –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
        if force_opus:
            models_to_try = ["anthropic/claude-opus-4.5"] + [
                m for m in self.models if m != "anthropic/claude-opus-4.5"
            ]
        else:
            models_to_try = self.models

        for model in models_to_try:
            try:
                start_time = time.time()
                model_name = _get_model_name(model)
                print(f"ü§ñ [{model_name} NO SYSTEM] –ù–∞—á–∏–Ω–∞—é —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞...")
                
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "user", "content": user_message}
                    ],
                    "max_tokens": 8000,
                    "temperature": 0.2,
                }

                timeout_value = 180 if 'opus' in model.lower() else 120
                print(f"üì° [{model_name} NO SYSTEM] –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ API...")
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=timeout_value)
                latency = time.time() - start_time

                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]

                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    input_tokens = result_data.get("usage", {}).get("prompt_tokens", tokens_used // 2)
                    output_tokens = result_data.get("usage", {}).get("completion_tokens", tokens_used // 2)
                    if input_tokens == 0 and output_tokens == 0:
                        input_tokens = tokens_used // 2
                        output_tokens = tokens_used // 2
                    
                    cost_info = calculate_cost(input_tokens, output_tokens, model)
                    model_type = "üß† OPUS" if "opus" in model.lower() else "ü§ñ SONNET" if "sonnet" in model.lower() else "‚ö° FLASH" if "gemini" in model.lower() or "flash" in model.lower() else "‚ùì UNKNOWN"
                    print(f"‚úÖ [{model_type}] [NO SYSTEM] –ú–æ–¥–µ–ª—å: {model_name}, Latency: {latency:.2f}—Å")
                    print(f"   üìä {format_cost_log(model, input_tokens, output_tokens, tokens_used)}")
                    log_api_call(model, True, latency, None)
                    track_model_usage(model, True, tokens_used)

                    self.model = model
                    return result
                elif response.status_code == 402:
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    print(f"‚ùå [{model_name} NO SYSTEM] {error_msg}. –ü—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    continue
                else:
                    error_msg = f"HTTP {response.status_code}"
                    log_api_error(model, latency, error_msg)
                    continue

            except requests.exceptions.Timeout:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = "–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (>300 —Å–µ–∫—É–Ω–¥)"
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ {model}")
                continue
            except Exception as e:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = handle_error(e, f"get_response_without_system ({model})", show_to_user=False)
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {model}: {e}")
                continue

        return "‚ùå –û—à–∏–±–∫–∞: –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –±–µ–∑ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞."
    
    def get_response_gemini_flash(
        self,
        user_message: str,
        context: str = "",
        use_flash_3: bool = True
    ) -> str:
        """
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Gemini Flash (2.5 –∏–ª–∏ 3.0)
        
        Args:
            user_message: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            use_flash_3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Flash 3.0 (True) –∏–ª–∏ Flash 2.5 (False). 
                        –ü—Ä–∏ –æ—à–∏–±–∫–µ Flash 3.0 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ fallback –Ω–∞ Flash 2.5
        
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç Gemini Flash
        """
        # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ Flash 3.0, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ, –∏–Ω–∞—á–µ Flash 2.5
        models_to_try = []
        if use_flash_3:
            models_to_try = [
                "google/gemini-3-flash-preview",      # Flash 3.0 Preview (–∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ OpenRouter)
                "google/gemini-3-flash",               # Flash 3.0 (–µ—Å–ª–∏ –ø–æ—è–≤–∏—Ç—Å—è –±–µ–∑ preview)
                "google/gemini-2.5-flash"              # Fallback –Ω–∞ Flash 2.5
            ]
        else:
            models_to_try = ["google/gemini-2.5-flash"]
        
        full_message = f"{context}\n\n–í–æ–ø—Ä–æ—Å: {user_message}" if context else user_message
        
        print(f"ü§ñ [‚ö° FLASH] [GEMINI FLASH TEXT] –ù–∞—á–∏–Ω–∞—é —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Gemini Flash...")
        
        # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –ø–æ –æ—á–µ—Ä–µ–¥–∏
        last_error = None
        for model in models_to_try:
            print(f"üì° [‚ö° FLASH] [GEMINI FLASH TEXT] –ü—Ä–æ–±—É—é –º–æ–¥–µ–ª—å: {model}")
            
            # Gemini –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç system_prompt —á–µ—Ä–µ–∑ OpenRouter
            payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": full_message}
                ],
                "max_tokens": 8000,
                "temperature": 0.2
            }
            
            try:
                start_time = time.time()
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]
                    
                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    input_tokens = result_data.get("usage", {}).get("prompt_tokens", tokens_used // 2)
                    output_tokens = result_data.get("usage", {}).get("completion_tokens", tokens_used // 2)
                    if input_tokens == 0 and output_tokens == 0:
                        input_tokens = tokens_used // 2
                        output_tokens = tokens_used // 2
                    
                    cost_info = calculate_cost(input_tokens, output_tokens, model)
                    log_api_call(model, True, latency, None)
                    track_model_usage(model, True, tokens_used)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    if "gemini-3-flash" in model:
                        model_name = "Gemini 3.0 Flash Preview" if "preview" in model else "Gemini 3.0 Flash"
                    else:
                        model_name = "Gemini 2.5 Flash"
                    print(f"‚úÖ [‚ö° FLASH] [GEMINI FLASH TEXT] –ú–æ–¥–µ–ª—å: {model_name}, Latency: {latency:.2f}—Å")
                    print(f"   üìä {format_cost_log(model, input_tokens, output_tokens, tokens_used)}")
                    log_api_success(model, latency, tokens_used, "GEMINI FLASH TEXT")
                    return result
                elif response.status_code == 404:
                    # –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é
                    error_msg = f"–ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ OpenRouter"
                    print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH TEXT] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                elif response.status_code == 402:
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    print(f"‚ùå [‚ö° FLASH] [GEMINI FLASH TEXT] {error_msg}")
                    return f"‚ùå –û—à–∏–±–∫–∞: {error_msg}"
                else:
                    # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH TEXT] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                    
            except requests.exceptions.Timeout:
                error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model} (>120 —Å–µ–∫—É–Ω–¥)"
                print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH TEXT] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
            except Exception as e:
                error_msg = handle_error(e, f"get_response_gemini_flash ({model})", show_to_user=False)
                print(f"‚ö†Ô∏è [‚ö° FLASH] [GEMINI FLASH TEXT] –û—à–∏–±–∫–∞ —Å {model}: {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        final_error = last_error or "–í—Å–µ –º–æ–¥–µ–ª–∏ Gemini Flash –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
        log_api_call(models_to_try[0] if models_to_try else "unknown", False, 0, final_error)
        track_model_usage(models_to_try[0] if models_to_try else "unknown", False)
        print(f"‚ùå [‚ö° FLASH] [GEMINI FLASH TEXT] {final_error}")
        return f"‚ùå –û—à–∏–±–∫–∞: {final_error}"
    
    def get_response_gemini_3(self, user_message: str, context: str = "") -> str:
        """
        –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Gemini 3.0 (–Ω–µ Flash) - –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            user_message: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç Gemini 3.0
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini 3.0 (–Ω–µ Flash) –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        models_to_try = [
            "google/gemini-3-pro-preview",      # Gemini 3.0 Pro Preview
            "google/gemini-3-pro",               # Gemini 3.0 Pro (–µ—Å–ª–∏ –ø–æ—è–≤–∏—Ç—Å—è)
            "google/gemini-2.5-pro",             # Fallback –Ω–∞ Gemini 2.5 Pro
            "google/gemini-3-flash-preview"     # Fallback –Ω–∞ Flash 3.0
        ]
        
        full_message = f"{context}\n\n–í–æ–ø—Ä–æ—Å: {user_message}" if context else user_message
        
        print(f"ü§ñ [üß† GEMINI 3.0] [GEMINI 3 TEXT] –ù–∞—á–∏–Ω–∞—é —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Gemini 3.0...")
        
        # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –ø–æ –æ—á–µ—Ä–µ–¥–∏
        last_error = None
        for model in models_to_try:
            print(f"üì° [üß† GEMINI 3.0] [GEMINI 3 TEXT] –ü—Ä–æ–±—É—é –º–æ–¥–µ–ª—å: {model}")
            
            # Gemini –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç system_prompt —á–µ—Ä–µ–∑ OpenRouter
            payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": full_message}
                ],
                "max_tokens": 8000,
                "temperature": 0.2
            }
            
            try:
                start_time = time.time()
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]
                    
                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    input_tokens = result_data.get("usage", {}).get("prompt_tokens", tokens_used // 2)
                    output_tokens = result_data.get("usage", {}).get("completion_tokens", tokens_used // 2)
                    if input_tokens == 0 and output_tokens == 0:
                        input_tokens = tokens_used // 2
                        output_tokens = tokens_used // 2
                    
                    cost_info = calculate_cost(input_tokens, output_tokens, model)
                    log_api_call(model, True, latency, None)
                    track_model_usage(model, True, tokens_used)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    if "gemini-3-pro" in model:
                        model_name = "Gemini 3.0 Pro Preview" if "preview" in model else "Gemini 3.0 Pro"
                    elif "gemini-2.5-pro" in model:
                        model_name = "Gemini 2.5 Pro"
                    else:
                        model_name = "Gemini 3.0 Flash Preview"
                    print(f"‚úÖ [üß† GEMINI 3.0] [GEMINI 3 TEXT] –ú–æ–¥–µ–ª—å: {model_name}, Latency: {latency:.2f}—Å")
                    print(f"   üìä {format_cost_log(model, input_tokens, output_tokens, tokens_used)}")
                    log_api_success(model, latency, tokens_used, "GEMINI 3 TEXT")
                    return result
                elif response.status_code == 404:
                    # –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é
                    error_msg = f"–ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ OpenRouter"
                    print(f"‚ö†Ô∏è [üß† GEMINI 3.0] [GEMINI 3 TEXT] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                elif response.status_code == 402:
                    error_msg = f"HTTP 402: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫—Ä–µ–¥–∏—Ç–æ–≤ –Ω–∞ OpenRouter –¥–ª—è –º–æ–¥–µ–ª–∏ {model}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    print(f"‚ùå [üß† GEMINI 3.0] [GEMINI 3 TEXT] {error_msg}")
                    return f"‚ùå –û—à–∏–±–∫–∞: {error_msg}"
                else:
                    # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ - –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                    print(f"‚ö†Ô∏è [üß† GEMINI 3.0] [GEMINI 3 TEXT] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                    last_error = error_msg
                    continue
                    
            except requests.exceptions.Timeout:
                error_msg = f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model} (>120 —Å–µ–∫—É–Ω–¥)"
                print(f"‚ö†Ô∏è [üß† GEMINI 3.0] [GEMINI 3 TEXT] {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
            except Exception as e:
                error_msg = handle_error(e, f"get_response_gemini_3 ({model})", show_to_user=False)
                print(f"‚ö†Ô∏è [üß† GEMINI 3.0] [GEMINI 3 TEXT] –û—à–∏–±–∫–∞ —Å {model}: {error_msg}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                last_error = error_msg
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
        final_error = last_error or "–í—Å–µ –º–æ–¥–µ–ª–∏ Gemini 3.0 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
        log_api_call(models_to_try[0] if models_to_try else "unknown", False, 0, final_error)
        track_model_usage(models_to_try[0] if models_to_try else "unknown", False)
        print(f"‚ùå [üß† GEMINI 3.0] [GEMINI 3 TEXT] {final_error}")
        return f"‚ùå –û—à–∏–±–∫–∞: {final_error}"
    
    def analyze_ecg_data(self, ecg_analysis: dict, user_question: str = None) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ –≠–ö–ì –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        –¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –∏–∑ claude_assistant.py (—Å—Ç—Ä–æ–∫–∏ 2408-2427)
        
        Args:
            ecg_analysis: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –≠–ö–ì
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            str: –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
        """
        context = f"""
üìä –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –≠–ö–ì:
‚Ä¢ –ß–∞—Å—Ç–æ—Ç–∞ —Å–µ—Ä–¥–µ—á–Ω—ã—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π: {ecg_analysis.get('heart_rate', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')} —É–¥/–º–∏–Ω
‚Ä¢ –†–∏—Ç–º: {ecg_analysis.get('rhythm_assessment', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω')}
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ QRS –∫–æ–º–ø–ª–µ–∫—Å–æ–≤: {ecg_analysis.get('num_beats', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')}
‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏: {ecg_analysis.get('duration', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')} —Å
‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–∞: {ecg_analysis.get('signal_quality', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ')}
"""
        
        question = user_question or """
–ö–∞–∫ –≤—Ä–∞—á-–∫–∞—Ä–¥–∏–æ–ª–æ–≥, –ø—Ä–æ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≠–ö–ì:
1. –û—Ü–µ–Ω–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ä–∏—Ç–º–∞ –∏ –ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç–∏
2. –í—ã—è–≤–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
3. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
4. –î–∞–π—Ç–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É –≤–µ–¥–µ–Ω–∏—é
"""
        return self.get_response(question, context)

