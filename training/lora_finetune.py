"""
lora_finetune.py - LoRA –¥–æ–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –º–æ–¥–µ–ª–∏

–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ MacBook.
–†–∞–±–æ—Ç–∞–µ—Ç ~6-7 –º–∏–Ω—É—Ç –Ω–∞ M1/M3 —Å 30-50 –ø—Ä–∏–º–µ—Ä–∞–º–∏.

–ó–∞–ø—É—Å–∫:
    python training/lora_finetune.py
"""

import torch
import json
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import warnings

warnings.filterwarnings("ignore")


def load_training_data(filepath: Path) -> Dataset:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç training data –∏–∑ jsonl —Ñ–∞–π–ª–∞"""
    examples = []

    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            example = json.loads(line)
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            text = f"""Instruction: {example['instruction']}

Input: {example['input']}

Output: {example['output']}"""
            examples.append({"text": text})

    if not examples:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None

    return Dataset.from_list(examples)


def setup_lora(model, lora_rank: int = 8, lora_alpha: int = 32):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –¥–ª—è –º–æ–¥–µ–ª–∏"""
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=lora_rank,
        lora_alpha=lora_alpha,
        lora_dropout=0.1,
        bias="none",
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    )

    model = get_peft_model(model, lora_config)
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())

    print(f"üéì LoRA –∫–æ–Ω—Ñ–∏–≥:")
    print(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    print(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"   % –æ–±—É—á–∞–µ–º—ã—Ö: {100 * trainable_params / total_params:.2f}%")

    return model


def finetune_flash(
    data_path: Path,
    model_name: str = "google/gemma-2-2b-it",
    output_dir: str = "data/models/flash_lora_v1",
    epochs: int = 3,
    batch_size: int = 1,
    learning_rate: float = 5e-4,
    max_length: int = 512,
):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è.

    Args:
        data_path: –ø—É—Ç—å –∫ training_data/flash_sft.jsonl
        model_name: –º–æ–¥–µ–ª—å –∏–∑ HF (gemma-2, mistral, llama –∏ —Ç.–¥.)
        output_dir: –≥–¥–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–µ—Å–∞
        epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (1 –¥–ª—è MacBook)
        learning_rate: —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """

    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
    dataset = load_training_data(data_path)

    if dataset is None or len(dataset) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            load_in_8bit=False
        )
        
        if not torch.cuda.is_available():
            # –î–ª—è CPU/MacBook
            model = model.to("cpu")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ HuggingFace")
        return None

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
    print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=max_length
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"]
    )

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
    print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA...")
    model = setup_lora(model, lora_rank=8, lora_alpha=32)

    # –û–±—É—á–∞–µ–º
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    training_args = TrainingArguments(
        output_dir=str(output_path),
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=1,
        learning_rate=learning_rate,
        weight_decay=0.01,
        save_strategy="epoch",
        logging_steps=5,
        save_total_limit=2,
        bf16=False,
        tf32=False,
        report_to=None,  # –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ wandb/tensorboard
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    )

    try:
        trainer.train()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        return None

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
    final_dir = Path(output_dir) / "final"
    final_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≤ {final_dir}...")
    model.save_pretrained(str(final_dir))
    tokenizer.save_pretrained(str(final_dir))

    print("‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –í–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {final_dir}")

    return model


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    data_file = Path("data/training_data/flash_sft.jsonl")

    if not data_file.exists():
        print(f"‚ùå –§–∞–π–ª {data_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ prepare_training_data.py")
        exit(1)

    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    print(f"üìÅ –î–∞–Ω–Ω—ã–µ: {data_file}")
    
    model = finetune_flash(
        data_path=data_file,
        output_dir="data/models/flash_lora_v1",
        epochs=3,
        batch_size=1,
        learning_rate=5e-4
    )
    
    if model:
        print("\n‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("üí° –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ training/inference.py")
    else:
        print("\n‚ùå –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ.")








