"""
inference.py - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
"""
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging

logger = logging.getLogger(__name__)


def load_finetuned_model(base_model_name: str, lora_path: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å + LoRA –≤–µ—Å–∞"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å: {base_model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
        )
        
        if not torch.cuda.is_available():
            model = model.to("cpu")

        # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º LoRA –≤–µ—Å–∞
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é LoRA –≤–µ—Å–∞: {lora_path}")
        model = PeftModel.from_pretrained(model, lora_path)

        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return model, tokenizer
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        raise


def generate_with_finetuned_model(
    input_text: str,
    model,
    tokenizer,
    max_length: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    instruction: Optional[str] = None
) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    
    if instruction is None:
        instruction = "You are an experienced physician. Analyze the following medical case and provide a structured diagnosis with differential diagnoses and recommendations."
    
    prompt = f"""Instruction: {instruction}

Input: {input_text}

Output:"""

    try:
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        if not torch.cuda.is_available():
            inputs = {k: v.to("cpu") for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ "Output:")
        if "Output:" in response:
            response = response.split("Output:")[-1].strip()
        
        return response
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"


# –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Streamlit
_model_cache = {}
_tokenizer_cache = {}


def get_finetuned_model_cached(base_model: str, lora_path: str):
    """
    –ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    
    –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Streamlit –º–æ–∂–Ω–æ –æ–±–µ—Ä–Ω—É—Ç—å –≤ @st.cache_resource
    """
    cache_key = f"{base_model}_{lora_path}"
    
    if cache_key not in _model_cache:
        model, tokenizer = load_finetuned_model(base_model, lora_path)
        _model_cache[cache_key] = model
        _tokenizer_cache[cache_key] = tokenizer
    
    return _model_cache[cache_key], _tokenizer_cache[cache_key]


def get_finetuned_model_simple(base_model: str, lora_path: str):
    """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –±–µ–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    return load_finetuned_model(base_model, lora_path)

