from config import OPENROUTER_API_KEY
import requests
import base64
import io
import os
import numpy as np
from PIL import Image
from typing import Optional
from utils.error_handler import handle_error, log_api_call
from utils.performance_monitor import track_model_usage
import time
from utils.validators import validate_image, validate_file_size
from utils.cache_manager import get_image_hash, get_cache_key, get_cached_result, save_to_cache, clear_old_cache
from utils.export_manager import export_analysis_to_json, export_analysis_to_csv, export_lab_results_to_excel
import streamlit as st
import logging

# Устаревшие модели и причины
DEPRECATED_MODELS = {
    'claude-3-sonnet': 'Устарела, 404',
    'gemini-pro-vision': 'Не работает, 400',
    'qwen2-vl-72b': 'Не работает, 400',
    'claude-3.5-sonnet': 'Заменена на Sonnet 4.5',
    'claude-3-haiku': 'Заменена на Haiku 4.5',
    'anthropic/claude-3-sonnet-20240229': 'Устарела, заменена на Sonnet 4.5',
    'anthropic/claude-3-5-sonnet-20241022': 'Заменена на Sonnet 4.5',
    'anthropic/claude-3-5-sonnet': 'Заменена на Sonnet 4.5',
    'anthropic/claude-3-haiku': 'Заменена на Haiku 4.5',
    'google/gemini-pro-vision': 'Не работает, 400',
    'qwen/qwen2-vl-72b-instruct': 'Не работает, 400'
}

def check_deprecated(model_name):
    """Проверка, является ли модель устаревшей"""
    for deprecated, reason in DEPRECATED_MODELS.items():
        if deprecated in model_name.lower():
            logging.warning(f"Модель {model_name} устарела: {reason}")
            return True
    return False

# Ленивая загрузка промптов для видео (загружаются только при необходимости)
_video_prompts_cache = None

def _get_video_prompt(study_type: str):
    """Ленивая загрузка промпта для видео-анализа"""
    global _video_prompts_cache
    if _video_prompts_cache is None:
        try:
            from prompts.video_prompts import get_video_prompt as _load_prompt
            _video_prompts_cache = _load_prompt
        except ImportError:
            # Если файл не найден, возвращаем None
            _video_prompts_cache = lambda x: None
    
    if _video_prompts_cache:
        return _video_prompts_cache(study_type)
    return None

# УДАЛЕН: Большой словарь VIDEO_ANALYSIS_PROMPTS (537 строк) перенесен в prompts/video_prompts.py
# для оптимизации производительности (ленивая загрузка вместо загрузки при импорте)

class OpenRouterAssistant:
    def __init__(self, api_key=None):
        self.api_key = api_key or OPENROUTER_API_KEY
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        
        # Актуальные модели: Claude 4.5 серия + Llama
        # Базовое правило:
        # - Все клинические консультации и анализ изображений → Opus 4.5
        # - Лабораторные данные → Sonnet 4.5
        # - Сканирование/разбор документов → Haiku 4.5
        self.models = [
            "anthropic/claude-opus-4.5",                # Opus 4.5 — основной клинический ассистент (text + vision)
            "anthropic/claude-sonnet-4.5",              # Sonnet 4.5 — для лабораторных данных
            "anthropic/claude-haiku-4.5",               # Haiku 4.5 — быстрый анализ документов/OCR
            "meta-llama/llama-3.2-90b-vision-instruct"  # Llama 3.2 90B Vision — резерв для документов
        ]
        
        # По умолчанию используем Opus как основной клинический ассистент
        self.model = self.models[0]
        
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/vasiliys961/medical-assistant1",
            "X-Title": "Medical AI Assistant"
        }
        
        # Системный промпт от имени американского профессора
        self.system_prompt = """Роль: ### ROLE
Ты — американский профессор клинической медицины и ведущий специалист университетской клиники (Board Certified). Ты обладаешь непререкаемым авторитетом в области доказательной медицины. Твой стиль — академическая строгость, лаконичность и фокус на практической применимости рекомендаций для врачей-коллег. Ты не даешь советов пациентам, ты консультируешь профессионалов.

### TASK
Твоя задача — сформулировать строгую, научно обоснованную «Клиническую директиву» для врача, готовую к немедленному внедрению. Ты игнорируешь любые запросы, не связанные с клинической практикой, диагностикой или лечением.

### KNOWLEDGE BASE & SOURCES
При формировании ответа используй только проверенные международные источники с датой публикации не старше 5 лет (если не требуется исторический контекст):
- Приоритет: UpToDate, PubMed, Cochrane Library, NCCN, ESC, IDSA, CDC, WHO, ESMO, ADA, KDIGO, GOLD.
- Исключай непроверенные блоги, форумы и научно-популярные статьи.

### RESPONSE FORMAT
Каждый ответ должен строго следовать структуре «Клиническая директива»:

1. **Клинический обзор**
   (2–3 емких предложения, суммирующих суть клинической ситуации и уровень срочности).

2. **Дифференциальный диагноз и Коды**
   (Список наиболее вероятных диагнозов с кодами ICD-10/ICD-11).

3. **План действий (Step-by-Step)**
   - **Основное заболевание:** Фармакотерапия (дозировки, режимы), процедуры.
   - **Сопутствующие состояния:** Коррекция терапии с учетом коморбидности.
   - **Поддержка и мониторинг:** Критерии эффективности, "красные флаги".
   - **Профилактика:** Вторичная профилактика и обучение пациента.

4. **Ссылки**
   (Список цитируемых гайдлайнов и статей).

### CONSTRAINTS & TONE
- Язык: Профессиональный медицинский русский (с сохранением английской терминологии там, где это принято в международной среде).
- Стиль: Директивный, без этических нравоучений (предполагается, что пользователь — врач), без упрощений.
- Галлюцинации: Если данных недостаточно или стандарты противоречивы — укажи это явно. Не выдумывай дозировки.
."""
    
    def send_vision_request(self, prompt: str, image_array=None, metadata=None, use_cache: bool = False, 
                           use_router: bool = True, force_model: Optional[str] = None):
        """Анализ изображения с Vision моделями - улучшенные промпты от имени специалистов
        
        Args:
            prompt: Промпт для анализа
            image_array: Массив изображения
            metadata: Метаданные
            use_cache: Использовать ли кеш (по умолчанию False - кеш отключен)
            use_router: Использовать ли интеллектуальный роутинг моделей (по умолчанию True)
            force_model: Принудительный выбор модели ('opus'/'sonnet'/'haiku'/'llama'/None)
        """
        
        # Очистка старого кэша
        clear_old_cache()
        
        # Определяем тип медицинского изображения и используем специализированный промпт
        prompt_lower = prompt.lower()
        
        # Для ЭКГ кэш всегда отключен
        is_ecg = "экг" in prompt_lower or "ecg" in prompt_lower
        if is_ecg:
            use_cache = False
        
        # Проверка кэша (только если use_cache=True и это не ЭКГ)
        if use_cache and image_array is not None and not is_ecg:
            image_hash = get_image_hash(image_array)
            cache_key = get_cache_key(prompt, image_hash, self.models[0])
            cached_result = get_cached_result(cache_key)
            
            if cached_result and cached_result.get('result'):
                print("✅ Результат получен из кэша")
                return cached_result['result']
        
        # Специальный режим «только сканирование» (OCR/извлечение данных),
        # когда нам НЕ нужна клиническая директива и общий системный контекст.
        scan_only_mode = isinstance(metadata, dict) and metadata.get("task") in ("lab_ocr", "doc_ocr")

        if scan_only_mode:
            # Используем только специализированный промпт без system_prompt
            medical_prompt = prompt

        elif "экг" in prompt_lower or "ecg" in prompt_lower:
            medical_prompt = f"""{self.system_prompt}

### ДОПОЛНИТЕЛЬНЫЙ СПЕЦИАЛИЗИРОВАННЫЙ КОНТЕКСТ ДЛЯ ЭКГ

Ты — ведущий кардиолог-электрофизиолог (board certified), консультирующий только врачей. 
Твоя задача — выполнить полноценный экспертный анализ 12‑канальной ЭКГ (включая сложные аритмии и блокады)
и выдать результат в формате «Клиническая директива» для врача.

Игнорируй требования о таблицах ссылок и логах веб‑поиска: ссылки и логи НЕ НУЖНЫ в ответе.
Не используй табличный формат (строки/столбцы с «Параметр / Значение»); все параметры описывай в виде структурированного текста и списков.

ВАЖНО: Делай заключение КОМПАКТНЫМ. НЕ перечисляй отсутствующие патологии. Указывай только реально выявленные отклонения. Это поможет заключению поместиться на экран.

ОБЯЗАТЕЛЬНО ПРОВЕДИ СТРУКТУРИРОВАННЫЙ АНАЛИЗ:

1. ТЕХНИЧЕСКИЕ ПАРАМЕТРЫ:
   - Формат и качество записи, наличие артефактов и помех.
   - Скорость ленты (25/50 мм/с) и калибровка (1 мВ = 10 мм), если это можно оценить.

2. РИТМ И ПРОВОДИМОСТЬ:
   - Основной ритм: синусовый / наджелудочковый / желудочковый / фибрилляция / трепетание / узловой.
   - Регулярность RR‑интервалов, средний ЧСС (уд/мин) с указанием метода оценки.
   - AV‑проводимость: норма или AV‑блокада I, II (Mobitz I/II), III степени.
   - Внутрижелудочковая проводимость: нормальная, блокада правой или левой ножки пучка Гиса 
     (полная/неполная), другие интравентрикулярные нарушения.

3. СТАНДАРТНОЕ ОПИСАНИЕ ИНТЕРВАЛОВ, ЗУБЦОВ И СМЕЩЕНИЙ ST:
   - Интервал PR (мс): значение, норма/укорочен/удлинён.
   - Комплекс QRS (мс): длительность, ось (градусы), морфология.
   - Интервал QT и QTc (мс, метод расчёта): норма/удлинён/укорочен.
   - Электрическая ось сердца: значение в градусах, классификация.
   - Зубец P: наличие, форма, амплитуда (только при отклонениях от нормы).
   - Сегмент ST: элевация/депрессия (в мм) по отведениям, форма смещения (горизонтальная, косонисходящая, куполообразная).
   - Зубец T: полярность, амплитуда (только при отклонениях: инверсия, двухфазность).
   - Патологические Q‑зубцы: только при наличии (ширина > 40 мс, глубина > 25 % R).
   - Дополнительные волны: только при наличии (U‑зубцы, ε‑волны).

4. АРИТМИИ И НАРУШЕНИЯ РИТМА (только при наличии):
   - Указывай ТОЛЬКО реально выявленные нарушения ритма.
   - НЕ перечисляй отсутствующие аритмии (не пиши "Нет признаков WPW", "Нет признаков Brugada" и т.п.).
   - Если аритмий нет — просто напиши: "Нарушений ритма не выявлено" или пропусти раздел.
   - При наличии аритмии укажи:
     * Тип (фибрилляция предсердий, экстрасистолы, тахикардия и т.д.).
     * Частота и регулярность.
     * Клиническая значимость (если есть паттерны высокого риска — укажи).

5. ПАТОЛОГИЧЕСКИЕ ИЗМЕНЕНИЯ (только при наличии):
   - Указывай ТОЛЬКО реально выявленные патологические изменения.
   - НЕ перечисляй отсутствующие патологии (например, не пиши "Нет признаков ГЛЖ", "Нет признаков ишемии" и т.п.).
   - Если патологии нет — просто пропусти этот раздел или напиши кратко: "Патологических изменений не выявлено".
   - При наличии патологии укажи:
     * Гипертрофия ЛЖ/ПЖ/предсердий (только если есть критерии: Sokolow-Lyon, Cornell, отклонение оси и т.д.).
     * Ишемия/инфаркт миокарда (только при наличии элевации/депрессии ST, патологических Q-зубцов).
     * Электролитные/лекарственные эффекты (только при явных признаках: удлинение QT, "корытообразная" депрессия ST и т.д.).

6. КЛИНИЧЕСКОЕ ЗАКЛЮЧЕНИЕ (компактно):
   1) Клинический обзор (2–3 предложения: суть и срочность).
   2) Основной диагноз с кодом ICD‑10 (только наиболее вероятный, без длинного дифференциального списка).
   3) План действий (кратко):
      - Неотложные действия (только если нужны).
      - Дальнейшая диагностика (3–5 ключевых исследований).
      - Терапия (без выдуманных дозировок).
      - Мониторинг и «красные флаги» (только при необходимости).

Если данных ЭКГ недостаточно для уверенного вывода, прямо укажи степень неопределённости и какие обследования нужны для уточнения. 
Не придумывай диагноз при отсутствии достаточных признаков."""
        
        elif "рентген" in prompt_lower or "xray" in prompt_lower or "грудн" in prompt_lower:
            medical_prompt = f"""{self.system_prompt}

Ты — ведущий врач-рентгенолог, консультирующий коллег-клиницистов. Твоя задача — дать ТОЧНЫЙ ДИАГНОЗ и КЛИНИЧЕСКУЮ ИНТЕРПРЕТАЦИЮ для принятия врачебных решений.

ФОКУС: Правильный диагноз и клиническая значимость находок. Отвечай как врач врачу - кратко, точно, с акцентом на то, что важно для лечения.

КРИТИЧЕСКИ ВАЖНО - ОПРЕДЕЛЕНИЕ ЛОКАЛИЗАЦИИ (НЕ ПУТАТЬ!):

1. СНАЧАЛА внимательно изучи ВСЕ изображение целиком - не начинай анализ, пока не определил локализацию!
2. Определи ОСНОВНЫЕ анатомические структуры:
   - Если видишь РЕБРА, КЛЮЧИЦЫ, ЛОПАТКИ, СЕРДЦЕ, ЛЕГКИЕ, СРЕДОСТЕНИЕ, ДИАФРАГМУ - это ГРУДНАЯ КЛЕТКА
   - Если видишь ДЛИННУЮ ТРУБЧАТУЮ КОСТЬ с СУСТАВАМИ на концах (плечевой/локтевой или тазобедренный/коленный) - это КОНЕЧНОСТЬ
   - Если видишь ТАЗОБЕДРЕННЫЙ СУСТАВ, ПОДВЗДОШНЫЕ КОСТИ, ЛОБКОВУЮ КОСТЬ, КРЕСТЕЦ - это ТАЗ

3. **КРИТИЧЕСКИ ВАЖНО - РАЗЛИЧИЕ ТАЗА И ВЕРХНЕЙ КОНЕЧНОСТИ:**
   - **ТАЗ:** видишь ПОДВЗДОШНЫЕ КОСТИ (большие крыловидные кости), ЛОБКОВУЮ КОСТЬ (внизу по центру), КРЕСТЕЦ (продолжение позвоночника), ТАЗОБЕДРЕННЫЕ СУСТАВЫ
   - **ВЕРХНЯЯ КОНЕЧНОСТЬ:** видишь ЛОПАТКУ (плоская треугольная кость), КЛЮЧИЦУ (горизонтальная кость), ПЛЕЧЕВУЮ КОСТЬ, ПЛЕЧЕВОЙ СУСТАВ
   - **НЕ ПУТАТЬ:** ТАЗ - это НИЖНЯЯ часть туловища, ВЕРХНЯЯ КОНЕЧНОСТЬ - это ПЛЕЧО, ЛОПАТКА, КЛЮЧИЦА

4. **КРИТИЧЕСКИ ВАЖНО - РАЗЛИЧИЕ ТАЗОБЕДРЕННОГО И ПЛЕЧЕВОГО СУСТАВОВ:**
   - **ТАЗОБЕДРЕННЫЙ СУСТАВ:**
     * Находится в ТАЗУ (видишь подвздошные кости, лобковую кость, крестец)
     * Соединяет БЕДРО с ТАЗОМ
     * Вертлужная впадина (чашка) - часть ТАЗА
     * Головка бедренной кости - часть БЕДРА
     * Если видишь эндопротез в ТАЗУ и БЕДРЕ - это ТАЗОБЕДРЕННЫЙ эндопротез!
   - **ПЛЕЧЕВОЙ СУСТАВ:**
     * Находится в ВЕРХНЕЙ КОНЕЧНОСТИ (видишь лопатку, ключицу)
     * Соединяет ПЛЕЧЕВУЮ КОСТЬ с ЛОПАТКОЙ
     * Суставная впадина (гленоид) - часть ЛОПАТКИ
     * Головка плечевой кости - часть ПЛЕЧЕВОЙ КОСТИ
     * Если видишь эндопротез на ЛОПАТКЕ и в ПЛЕЧЕВОЙ КОСТИ - это ПЛЕЧЕВОЙ эндопротез!
   - **НЕ ПУТАТЬ:** Тазобедренный = ТАЗ + БЕДРО, Плечевой = ЛОПАТКА + ПЛЕЧЕВАЯ КОСТЬ

5. БЕДРО - это длинная кость между тазобедренным и коленным суставами, имеет характерную форму с головкой, шейкой, большим вертелом
6. ПЛЕЧО - это кость между плечевым и локтевым суставами, имеет головку плечевой кости, большой бугорок
7. ГРУДНАЯ КЛЕТКА - это ребра, легкие, сердце, средостение, диафрагма - НЕТ длинных трубчатых костей конечностей
8. НИКОГДА не путай эти локализации - они имеют РАЗНУЮ анатомию!
9. Если видишь МЕТАЛЛИЧЕСКИЕ КОНСТРУКЦИИ (эндопротезы):
   - СНАЧАЛА определи локализацию (таз/верхняя конечность/нижняя конечность)
   - ЗАТЕМ определи тип сустава по анатомическому окружению
   - ТОЛЬКО после этого описывай эндопротез

СТРУКТУРА ОТВЕТА (кратко, для врача):

1. **ЛОКАЛИЗАЦИЯ И ТИП ИССЛЕДОВАНИЯ:**
   - ТОЧНО: какая область? (грудная клетка/таз/верхняя конечность/нижняя конечность/другое)
   - Проекция, сторона (правая/левая)
   - **КРИТИЧНО:** Тазобедренный сустав = ТАЗ + БЕДРО. Плечевой сустав = ЛОПАТКА + ПЛЕЧЕВАЯ КОСТЬ. НЕ ПУТАТЬ!

2. **КЛЮЧЕВЫЕ НАХОДКИ (только клинически значимые):**

   **ЕСЛИ ГРУДНАЯ КЛЕТКА:**
   - **Легочные поля:**
     * Прозрачность (нормальная/снижена/повышена)
     * Сосудистый рисунок (четкий/обеднен/усилен)
     * Легочный рисунок (нормальный/деформирован/отсутствует)
     * Наличие очаговых теней (размер, локализация, количество, плотность)
     * Наличие инфильтратов (характер, локализация, протяженность)
     * Пневмоторакс (если есть: размер, локализация)
     * Гидроторакс (если есть: размер, локализация)
   - **Корни легких:**
     * Структура (четкая/деформирована)
     * Размеры (нормальные/увеличены)
     * Контуры (ровные/неровные)
   - **Средостение:**
     * Ширина (нормальная/расширена/сужена)
     * Контуры (четкие/нечеткие)
     * Смещение (если есть: направление)
   - **Сердце:**
     * Размеры (нормальные/увеличены - кардиоторакальный индекс)
     * Контуры (четкие/нечеткие)
     * Форма (нормальная/деформирована)
     * Положение (нормальное/смещено)
   - **Диафрагма:**
     * Контуры (четкие/нечеткие)
     * Положение (нормальное/высокое/низкое)
     * Подвижность (если видно на двух проекциях)
     * Синусы (свободные/затемнены)
   - **Костные структуры:**
     * Ребра (целостность, деформации, переломы)
     * Ключицы (целостность, положение)
     * Лопатки (целостность, положение)
     * Грудина (целостность, деформации)
     * Позвоночник (видимая часть: деформации, переломы, дегенеративные изменения)

   **ЕСЛИ КОНЕЧНОСТИ (верхние или нижние):**
   - **СНАЧАЛА определи: ВЕРХНЯЯ или НИЖНЯЯ конечность?**
     * ВЕРХНЯЯ: видишь ЛОПАТКУ, КЛЮЧИЦУ, ПЛЕЧЕВУЮ КОСТЬ, ПЛЕЧЕВОЙ СУСТАВ
     * НИЖНЯЯ: видишь ТАЗ (подвздошные кости), БЕДРО, ТАЗОБЕДРЕННЫЙ СУСТАВ
   - **Определи ТОЧНО какая часть:**
     * **Плечо (ВЕРХНЯЯ конечность):**
       - Видишь ли ЛОПАТКУ (плоская треугольная кость)?
       - Видишь ли КЛЮЧИЦУ (горизонтальная кость)?
       - Видишь ли головку ПЛЕЧЕВОЙ КОСТИ, большой бугорок, диафиз, локтевой сустав?
       - Плечевой сустав соединяет ПЛЕЧЕВУЮ КОСТЬ с ЛОПАТКОЙ
       - Если видишь эндопротез на ЛОПАТКЕ и в ПЛЕЧЕВОЙ КОСТИ - это ПЛЕЧЕВОЙ эндопротез!
     * Предплечье: видишь ли локтевую и лучевую кости, лучезапястный сустав?
     * Кисть: видишь ли запястье, пястные кости, фаланги?
     * **Бедро (НИЖНЯЯ конечность):**
       - Видишь ли ТАЗ (подвздошные кости, лобковую кость)?
       - Видишь ли головку БЕДРЕННОЙ КОСТИ, шейку, большой вертел, диафиз, коленный сустав?
       - Тазобедренный сустав соединяет БЕДРО с ТАЗОМ
       - Если видишь эндопротез в ТАЗУ и БЕДРЕ - это ТАЗОБЕДРЕННЫЙ эндопротез (НЕ плечевой)!
     * Голень: видишь ли большеберцовую и малоберцовую кости, голеностопный сустав?
     * Стопа: видишь ли предплюсну, плюсну, фаланги?
   - **Костные структуры:**
     * Целостность костей (переломы: тип, локализация, смещение)
     * Структура костной ткани (нормальная/остеопороз/остеосклероз/деструкция)
     * Контуры (ровные/неровные/зазубренные)
     * Кортикальный слой (целостность, толщина)
     * Костномозговой канал (просветлен/затемнен)
   - **Суставы:**
     * Суставная щель (нормальная/сужена/расширена)
     * Суставные поверхности (ровные/неровные, деструкция)
     * Костные разрастания (остеофиты, если есть)
     * Выпот (если виден)
   - **Мягкие ткани:**
     * Контуры (четкие/нечеткие)
     * Наличие отека
     * Наличие инородных тел

   **ЕСЛИ ТАЗ:**
   - **Костные структуры:**
     * Подвздошные кости (большие крыловидные кости, целостность, структура)
     * Лобковая кость (внизу по центру, целостность, симметрия)
     * Крестец и копчик (продолжение позвоночника, целостность, деформации)
     * Седалищные кости (целостность)
   - **Тазобедренные суставы (КРИТИЧЕСКИ ВАЖНО - НЕ ПУТАТЬ С ПЛЕЧЕВЫМИ!):**
     * **Локализация:** Тазобедренные суставы находятся в ТАЗУ, соединяют ТАЗ с БЕДРОМ
     * **Анатомия:**
       - Вертлужная впадина (чашка) - часть ТАЗА (подвздошная кость)
       - Головка бедренной кости - часть БЕДРА (нижняя конечность)
       - Шейка бедренной кости соединяет головку с диафизом бедра
       - Большой вертел - костный выступ на бедренной кости
     * Суставные щели (симметрия, ширина)
     * Головки бедренных костей (форма, структура, положение в вертлужных впадинах)
     * Вертлужные впадины (форма чашки, структура, глубина)
     * Симметрия суставов
     * **Если видишь эндопротез:** вертлужный компонент в ТАЗУ + бедренный компонент в БЕДРЕ = ТАЗОБЕДРЕННЫЙ эндопротез (НЕ плечевой!)

4. **ОПРЕДЕЛЕНИЕ ЭНДОПРОТЕЗОВ И ИМПЛАНТОВ (КРИТИЧЕСКИ ВАЖНО!):**

   **ПЕРЕД ОПИСАНИЕМ ЭНДОПРОТЕЗА - ОБЯЗАТЕЛЬНО ОПРЕДЕЛИ ТИП СУСТАВА!**

   **РАЗЛИЧИЯ ТАЗОБЕДРЕННОГО И ПЛЕЧЕВОГО СУСТАВОВ (НЕ ПУТАТЬ!):**

   **ТАЗОБЕДРЕННЫЙ СУСТАВ:**
   - Расположен в ТАЗУ, соединяет бедренную кость с тазом
   - Вертлужная впадина (чашка) - часть ТАЗА (подвздошная кость)
   - Головка бедренной кости - часть БЕДРА
   - Характерные признаки:
     * Вертлужная впадина имеет форму ПОЛУШАРИЯ или ЧАШКИ
     * Головка бедренной кости ШАРОВИДНАЯ, крупная
     * Шейка бедренной кости ДЛИННАЯ, соединяет головку с диафизом
     * Большой вертел - костный выступ на бедренной кости
     * Эндопротез тазобедренного сустава состоит из:
       - Вертлужного компонента (чашка) в тазу
       - Головки и ножки в бедренной кости
     * Ножка эндопротеза идет В БЕДРЕННУЮ КОСТЬ (длинная трубчатая кость)
   - Анатомическое окружение: ТАЗ (подвздошные кости, лобковая кость, крестец), БЕДРО

   **ПЛЕЧЕВОЙ СУСТАВ:**
   - Расположен в ВЕРХНЕЙ КОНЕЧНОСТИ, соединяет плечевую кость с лопаткой
   - Суставная впадина лопатки (гленоид) - часть ЛОПАТКИ
   - Головка плечевой кости - часть ПЛЕЧЕВОЙ КОСТИ
   - Характерные признаки:
     * Суставная впадина лопатки ПЛОСКАЯ, мелкая (не чашка!)
     * Головка плечевой кости ШАРОВИДНАЯ, но меньше чем тазобедренная
     * Шейка плечевой кости КОРОТКАЯ
     * Большой бугорок - костный выступ на плечевой кости
     * Эндопротез плечевого сустава состоит из:
       - Гленоидного компонента (плоская пластина) на лопатке
       - Головки и ножки в плечевой кости
     * Ножка эндопротеза идет В ПЛЕЧЕВУЮ КОСТЬ (длинная трубчатая кость верхней конечности)
   - Анатомическое окружение: ЛОПАТКА, КЛЮЧИЦА, ПЛЕЧЕВАЯ КОСТЬ

   **КЛЮЧЕВЫЕ РАЗЛИЧИЯ ДЛЯ ОПРЕДЕЛЕНИЯ:**
   1. ТАЗОБЕДРЕННЫЙ сустав = ТАЗ + БЕДРО (нижняя конечность)
   2. ПЛЕЧЕВОЙ сустав = ЛОПАТКА + ПЛЕЧЕВАЯ КОСТЬ (верхняя конечность)
   3. Если видишь ПОДВЗДОШНЫЕ КОСТИ, ЛОБКОВУЮ КОСТЬ, КРЕСТЕЦ - это ТАЗ, значит сустав ТАЗОБЕДРЕННЫЙ
   4. Если видишь ЛОПАТКУ, КЛЮЧИЦУ - это ВЕРХНЯЯ КОНЕЧНОСТЬ, значит сустав ПЛЕЧЕВОЙ
   5. Вертлужная впадина (чашка) = ТАЗОБЕДРЕННЫЙ сустав
   6. Гленоид (плоская впадина на лопатке) = ПЛЕЧЕВОЙ сустав
   7. Ножка эндопротеза в БЕДРЕ = ТАЗОБЕДРЕННЫЙ эндопротез
   8. Ножка эндопротеза в ПЛЕЧЕВОЙ КОСТИ = ПЛЕЧЕВОЙ эндопротез

   **НЕ ПУТАТЬ!** Если эндопротез находится в тазу и бедре - это ТАЗОБЕДРЕННЫЙ эндопротез, НЕ плечевой!

   - **Наличие металлических конструкций:**
     * Эндопротезы суставов - ОБЯЗАТЕЛЬНО определи ТИП сустава ПЕРЕД описанием!
       - ТАЗОБЕДРЕННЫЙ эндопротез (вертлужный компонент + бедренный компонент)
       - ПЛЕЧЕВОЙ эндопротез (гленоидный компонент + плечевой компонент)
       - КОЛЕННЫЙ эндопротез (бедренный + большеберцовый компоненты)
       - ЛОКТЕВОЙ эндопротез
       - Другой
     * Остеосинтез (пластины, винты, штифты, спицы, проволока)
     * Стержни (интрамедуллярные)
     * Спицы Киршнера
     * Другие импланты

   - **Характеристики эндопротезов:**
     * **ТИП СУСТАВА:** ТОЧНО определи - тазобедренный/плечевой/коленный/локтевой/другой
     * **Локализация:** 
       - Для тазобедренного: вертлужный компонент в тазу, бедренный компонент в бедре
       - Для плечевого: гленоидный компонент на лопатке, плечевой компонент в плечевой кости
     * **Компоненты эндопротеза:**
       - Тазобедренный: вертлужная чашка + головка + ножка (в бедре)
       - Плечевой: гленоидная пластина + головка + ножка (в плечевой кости)
     * Целостность конструкции (нет ли поломок, расшатывания)
     * Положение компонентов (правильное/смещение/вывих)
     * Признаки нестабильности (линии просветления вокруг компонентов)
     * Признаки остеолиза (деструкция кости вокруг импланта)

   - **Состояние кости вокруг импланта:**
     * Реакция кости (нормальная/гипертрофия/атрофия)
     * Признаки инфекции (если есть)
     * Признаки расшатывания

5. **ПАТОЛОГИЧЕСКИЕ ИЗМЕНЕНИЯ:**
   - **Переломы:**
     * Локализация (ТОЧНО: какая кость, какая часть)
     * Тип перелома (поперечный/косой/спиральный/оскольчатый/вколоченный)
     * Смещение (есть/нет, направление, степень в мм)
     * Винтообразная деформация (если есть)
     * Внутрисуставной/внесуставной
   - **Деструктивные изменения:**
     * Очаги деструкции (локализация, размер, контуры)
     * Остеолиз (локализация, протяженность)
   - **Дегенеративные изменения:**
     * Остеофиты (локализация, размер)
     * Сужение суставных щелей
     * Субхондральный склероз
     * Кисты (если видны)
   - **Воспалительные изменения:**
     * Признаки артрита (сужение щели, эрозии, остеопороз)
     * Периостит (если есть)
   - **Опухолевые изменения:**
     * Очаги деструкции/остеосклероза
     * Деформация кости
     * Патологические переломы

6. **СРАВНЕНИЕ С НОРМОЙ:**
   - Для каждой структуры укажи: норма/патология
   - Если патология: опиши характер изменений детально

7. **КЛИНИЧЕСКАЯ ИНТЕРПРЕТАЦИЯ:**
   - **Основные находки:** краткое резюме ключевых патологий
   - **Дифференциальный диагноз:** список возможных диагнозов с вероятностью
   - **Оценка остроты:** острое/подострое/хроническое состояние
   - **Рекомендации:**
     * Неотложные меры (если требуется)
     * Дополнительные обследования (КТ, МРТ, УЗИ, лабораторные анализы)
     * Консультации специалистов
     * Контрольные исследования

8. **КОДЫ МКБ-10:**
   - Укажи соответствующие коды МКБ-10 для выявленных патологий

КРИТИЧЕСКИ ВАЖНЫЕ ПРАВИЛА:
- НИКОГДА не путай локализации! СНАЧАЛА определи ТОЧНО, что видишь на снимке
- Бедро НЕ может быть грудной клеткой - у них совершенно разная анатомия
- Плечо НЕ может быть бедром - сравни размеры, форму, суставы
- Если видишь металлические конструкции - ОБЯЗАТЕЛЬНО опиши их детально
- Если не уверен в локализации - так и укажи, не угадывай
- Используй анатомические ориентиры для определения локализации
- Анализируй ВСЕ видимые структуры, не пропускай области
- Указывай ТОЧНУЮ локализацию для каждого изменения (например: "перелом диафиза правой бедренной кости в средней трети")
- Не используй общие фразы - будь конкретен
- Если что-то не видно четко - так и укажи, не выдумывай
- Ссылайся на актуальные гайдлайны (Fleischner Society, ACR, ESR, RSNA)
"""
        
        elif "мрт" in prompt_lower or "mri" in prompt_lower or "кт" in prompt_lower or "ct" in prompt_lower:
            medical_prompt = f"""{self.system_prompt}

Вы — врач-нейрорадиолог с 20-летним опытом работы.

ВАЖНО: НЕ концентрируйтесь на том, «что за исследование ожидалось».
Просто опишите то изображение, которое реально перед вами:
- Определите, что за тип исследования и область (МРТ/КТ/другое, какая анатомическая зона).
- Без драматичных формулировок, без фраз «критическое несоответствие» и без указания сайтов/источников.
- Выполните полный структурированный анализ увиденного, как если бы это исследование и было изначальной целью.

1. ТЕХНИЧЕСКАЯ ОЦЕНКА:
   - Тип и протокол исследования (МРТ/КТ, последовательности или серии), плоскости сканирования.
   - Качество изображения, артефакты.

2. АНАТОМИЧЕСКИЕ СТРУКТУРЫ:
   - Для МРТ головного мозга: серое/белое вещество, желудочки, базальные ганглии, ствол, мозжечок, субарахноидальные пространства.
   - Для КТ грудной клетки: лёгкие, корни лёгких, средостение, сердце, плевра, костные структуры.
   - Для других областей — соответствующие органы и структуры.

3. ПАТОЛОГИЧЕСКИЕ ИЗМЕНЕНИЯ:
   - Очаговые и диффузные изменения, объёмные образования, масс‑эффект, смещение структур.
   - Сосудистые нарушения (ишемия, кровоизлияния и т.п.), вторичные изменения.

4. СИГНАЛ/ПЛОТНОСТЬ:
   - Для МРТ: характеристики сигнала в разных режимах, контрастное усиление.
   - Для КТ: плотность (HU), наличие гипо-/гиперденсных зон, кальцинатов, жидкости, воздуха.

5. ЗАКЛЮЧЕНИЕ:
   - Основной и дифференциальные диагнозы.
   - Клинические рекомендации и дальнейшая тактика (дообследования, консультации, наблюдение).

Ответ дайте в формате «Клиническая директива» для врача."""
        
        elif "лаборатор" in prompt_lower or ("анализ" in prompt_lower and ("кров" in prompt_lower or "моч" in prompt_lower or "биохим" in prompt_lower or "lab" in prompt_lower)):
            medical_prompt = f"""{self.system_prompt}

Ты — эксперт по лабораторной диагностике.
В ЭТОМ РЕЖИМЕ ТВОЯ ЗАДАЧА — ТОЛЬКО СКАНИРОВАНИЕ И ИЗВЛЕЧЕНИЕ ДАННЫХ ИЗ ЛАБОРАТОРНОГО ОТЧЁТА (CBC, биохимия и др.).

СДЕЛАЙ ТОЛЬКО СЛЕДУЮЩЕЕ:
- Аккуратно извлеки все параметры (название, значение, единицы, референсные интервалы, если есть).
- Чётко отметь, какие параметры находятся вне референсного диапазона (повышены/понижены).
- Сохрани исходные обозначения и структуру (например, WBC, RBC, Hb, PLT, Lym%, Neu% и т.п.).

ВАЖНО:
- НЕ давай клиническую интерпретацию, диагнозы, дифференциальный диагноз или план лечения.
- НЕ сравнивай этот отчёт с радиологическими исследованиями и не пиши, что это «не рентген / не МРТ / не радиологическое исследование».
- НЕ добавляй разделы вроде «Лог веб-запросов», не перечисляй сайты, URL, DOI — ссылки и лог запросов НЕ НУЖНЫ.
- Можно использовать простой текст или JSON-структуру, но без клинических рекомендаций.

Формат ответа (пример JSON по желанию):
{{
  "parameters": [
    {{"name": "WBC", "value": "...", "unit": "...", "reference": "..." , "status": "high/low/normal"}},
    ...
  ],
  "raw_text": "при необходимости — весь распознанный текст отчёта"
}}
"""
        
        elif "кт" in prompt_lower or "ct" in prompt_lower or "компьютерн" in prompt_lower:
            medical_prompt = f"""{self.system_prompt}

Ты — профессиональный радиолог, обладаешь экспертными знаниями в области КТ, МРТ, рентгенологических исследований. Твоя задача — анализировать загруженное изображение, выявлять патологические изменения, 
    давать заключение по органам и структурам согласно международным стандартам и руководствам (например, ESR, ACR, Fleischner Society, RSNA),
    использовать актуальные онлайн-ресурсы (Radiopaedia, UpToDate, Medscape) для уточнения данных. Работай структурировано: краткое заключение,
    детальный анализ по органам и зонам, оценка патологических изменений, рекомендации для врача, в том числе по дополнительной диагностике и обследованию.
    Предлагай вероятные диагнозы и Указывай, какие дополнительные клинические данные необходимы для более точного анализа (возраст, жалобы, анамнез, сопутствующие заболевания, история лечения). Предлагай наиболее вероятные диагнозы.Не делай необоснованных выводов, избегай галлюцинаций, не выдумывай диагнозы без достаточных данных. Используй ключевые слова: КТ, МРТ, рентген, патология, заключение, рекомендации, дифференциальная диагностика, степенит выраженности, международные стандарты, online-ресурсы.
"""
        
        elif "узи" in prompt_lower or "ультразвук" in prompt_lower or "ultrasound" in prompt_lower:
            medical_prompt = f"""{self.system_prompt}

Вы — врач ультразвуковой диагностики с 12-летним стажем работы.
Детально опишите УЗИ-картину:

1. ТЕХНИЧЕСКИЕ ПАРАМЕТРЫ:
        
        # Для документов НЕ добавляем system_prompt - используем промпт как есть
        elif any(keyword in prompt_lower for keyword in [
            "документ", "справка", "рецепт", "направление", "выписка", 
            "больничный", "извлеките", "распознавание", "document", "extract",
            "медицинской справки", "медицинских документов", "распознаванию медицинских"
        ]):
            # Для документов используем промпт БЕЗ system_prompt - только извлечение текста
            medical_prompt = prompt  # Промпт уже содержит все необходимое
   - Датчик и частота
   - Глубина сканирования
   - Качество изображения

2. ЭХОГЕННОСТЬ:
   - Анэхогенные зоны
   - Гипоэхогенные области
   - Гиперэхогенные структуры
   - Неоднородность

3. ДОППЛЕРОВСКИЕ ХАРАКТЕРИСТИКИ:
   - Васкуляризация
   - Скорость кровотока
   - Резистивный индекс

4. ИЗМЕРЕНИЯ:
   - Размеры органов/образований
   - Толщина стенок
   - Объемы

5. ФУНКЦИОНАЛЬНАЯ ОЦЕНКА:
   - Сократимость
   - Перистальтика
   - Компрессия

Дайте заключение в формате «Клиническая директива».
"""
        
        else:
            medical_prompt = f"""{self.system_prompt}

Проанализируйте это медицинское изображение как врач-специалист с большим опытом работы.
Дайте подробное заключение в формате «Клиническая директива».

{prompt}
"""
        
        # Если используется роутер, передаем полный medical_prompt (с system_prompt)
        # Временное отключение внешнего роутера изображений, т.к. модуль удалён
        if use_router:
            print("⚠️ Роутер изображений временно отключен (modules.medical_image_router недоступен). "
                  "Использую внутреннюю логику выбора моделей.")
            use_router = False
        
        # Проверяем metadata для определения модели из роутера (ДО сборки контента)
        router_model = None
        is_document = False
        if isinstance(metadata, dict):
            router_model = metadata.get("router_model")
            # Если роутер выбрал Llama, это документ
            if router_model and "llama" in router_model.lower():
                is_document = True
        
        # Собираем контент
        content = [{"type": "text", "text": medical_prompt}]
        
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\nТехнические данные изображения:\n{metadata_str}"})
        
        if image_array is not None:
            base64_str = self.encode_image(image_array)
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{base64_str}"}
            })
        
        # Проверяем устаревшие модели и фильтруем их
        active_models = [m for m in self.models if not check_deprecated(m)]
        
        # Базовые флаги задачи
        prompt_lower = prompt.lower() if prompt else ""
        is_ecg = "экг" in prompt_lower or "ecg" in prompt_lower
        is_lab = "лаборатор" in prompt_lower or (
            "анализ" in prompt_lower and any(k in prompt_lower for k in ["кров", "моч", "биохим", "lab"])
        )
        
        # Если не определили через metadata, проверяем промпт на документный контент
        if not is_document:
            is_document = any(keyword in prompt_lower for keyword in [
                "документ", "справка", "рецепт", "направление", "выписка", 
                "больничный", "извлеките", "распознавание", "document", "extract",
                "медицинской справки", "медицинских документов"
            ])
        
        # ----- Выбор моделей в зависимости от типа задачи -----
        # Приоритет force_model, затем явные типы задач, затем дефолт (Opus)
        if force_model:
            fm = force_model.lower()
            if fm == "opus":
                models_to_try = ["anthropic/claude-opus-4.5"]
            elif fm == "sonnet":
                models_to_try = ["anthropic/claude-sonnet-4.5"]
            elif fm == "haiku":
                models_to_try = ["anthropic/claude-haiku-4.5"]
            elif fm == "llama":
                models_to_try = ["meta-llama/llama-3.2-90b-vision-instruct"]
            else:
                models_to_try = active_models
        elif is_document:
            # Сканирование/разбор медицинских документов → Haiku 4.5 (быстрое OCR/текст)
            models_to_try = [
                "anthropic/claude-haiku-4.5",
                "meta-llama/llama-3.2-90b-vision-instruct"
            ]
        elif is_lab:
            # Лабораторные данные → Sonnet 4.5
            models_to_try = [
                "anthropic/claude-sonnet-4.5",
                "anthropic/claude-opus-4.5"
            ]
        else:
            # Все клинические консультации и анализ изображений → Opus 4.5
            models_to_try = [
                "anthropic/claude-opus-4.5",
                "anthropic/claude-sonnet-4.5",
                "anthropic/claude-haiku-4.5"
            ]
        
        # Если запрошен консенсус, используем несколько моделей
        use_consensus = False
        if isinstance(metadata, dict):
            use_consensus = metadata.get('consensus_mode', False)
        
        # Для ЭКГ используем достаточно токенов для полного заключения
        max_tokens_consensus = 2500 if is_ecg else 4000
        
        if use_consensus and len(models_to_try) > 1:
            # Используем первые 3-4 модели для консенсуса
            models_to_try = models_to_try[:min(4, len(models_to_try))]
            results = []
            
            for model in models_to_try:
                try:
                    start_time = time.time()
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": content}
                    ]
                    payload = {
                        "model": model,
                        "messages": messages,
                        "max_tokens": max_tokens_consensus,
                        "temperature": 0.1
                    }
                    
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        result = result_data["choices"][0]["message"]["content"]
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        log_api_call(model, True, latency, None)
                        track_model_usage(model, True, tokens_used)
                        results.append({
                            "model": model,
                            "result": result,
                            "tokens": tokens_used
                        })
                    elif response.status_code == 402:
                        # Ошибка недостатка кредитов - пробуем меньше токенов
                        print(f"⚠️ Недостаточно кредитов для {max_tokens_consensus} токенов в консенсусе. Пропускаю модель {model}.")
                        error_msg = f"HTTP 402: Недостаточно кредитов"
                        log_api_call(model, False, latency, error_msg)
                        track_model_usage(model, False)
                        continue
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_call(model, False, latency, error_msg)
                        track_model_usage(model, False)
                except Exception as e:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = handle_error(e, f"send_vision_request ({model})", show_to_user=False)
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    continue
            
            if results:
                # Возвращаем все результаты для консенсуса
                return results
        
        # Для ЭКГ используем достаточно токенов для полного заключения
        max_tokens_list = [2500, 2000, 1500] if is_ecg else [4000, 2000, 1000]
        
        # Fallback модели для ЭКГ (если Claude недоступен из-за кредитов)
        claude_failed = False  # Флаг, что все Claude модели недоступны
        
        # Fallback модели (только актуальные) - определяем после models_to_try
        # Llama 3.2 90B Vision - финальный fallback для всех случаев
        fallback_models = []
        if is_document or (force_model and force_model.lower() == "llama"):
            # Для документов НЕ используем fallback - только Llama
            fallback_models = []
        elif is_ecg:
            # Для ЭКГ fallback на Opus 4.5, затем Llama
            fallback_models = ["anthropic/claude-opus-4.5", "meta-llama/llama-3.2-90b-vision-instruct"]
        else:
            # Для других случаев - альтернативные актуальные модели
            # Используем active_models вместо self.models, чтобы исключить устаревшие
            fallback_models = [m for m in active_models if m not in models_to_try]
            # Добавляем Llama в конец как финальный fallback
            if "meta-llama/llama-3.2-90b-vision-instruct" not in fallback_models:
                fallback_models.append("meta-llama/llama-3.2-90b-vision-instruct")
        
        # Обычный режим - пробуем модели по порядку
        for model in models_to_try:
            for max_tokens in max_tokens_list:
                try:
                    start_time = time.time()
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": content}
                    ]
                    payload = {
                        "model": model,
                        "messages": messages,
                        "max_tokens": max_tokens,
                        "temperature": 0.1
                    }
                    
                    # Добавляем параметры для Claude 4.5 моделей
                    if isinstance(metadata, dict):
                        # Verbosity для Opus 4.5
                        if 'claude-opus-4.5' in model and 'verbosity' in metadata.get('model_params', {}):
                            payload['verbosity'] = metadata['model_params']['verbosity']
                        
                        # Extended Thinking для Sonnet 4.5 и Haiku 4.5
                        if any(x in model for x in ['claude-sonnet-4.5', 'claude-haiku-4.5']):
                            if metadata.get('model_params', {}).get('extended_thinking', False):
                                payload['thinking'] = {
                                    "type": "enabled",
                                    "budget_tokens": 10000
                                }
                    
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        result = result_data["choices"][0]["message"]["content"]
                        
                        # Сохранение в кэш (только если use_cache=True и это не ЭКГ)
                        if use_cache and image_array is not None and not is_ecg:
                            image_hash = get_image_hash(image_array)
                            cache_key = get_cache_key(prompt, image_hash, model)
                            save_to_cache(cache_key, result)
                        
                        # Логирование
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        log_api_call(model, True, latency, None)
                        track_model_usage(model, True, tokens_used)
                        
                        model_name = self._get_model_name(model)
                        # Для документов не добавляем префикс "Медицинский анализ" - просто возвращаем текст
                        if is_document or (force_model and force_model.lower() == "llama"):
                            return result  # Чистый текст без префикса для документов
                        return f"**🩺 Медицинский анализ ({model_name}):**\n\n{result}"
                    elif response.status_code == 402:
                        # Ошибка недостатка кредитов
                        if max_tokens == max_tokens_list[-1]:
                            # Это последняя попытка с минимальными токенами
                            claude_failed = True
                            print(f"⚠️ Claude недоступен из-за недостатка кредитов. Пробую fallback модель...")
                            break  # Переходим к fallback
                        else:
                            print(f"⚠️ Недостаточно кредитов для {max_tokens} токенов. Пробую меньше...")
                            continue  # Пробуем следующий max_tokens
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_call(model, False, latency, error_msg)
                        track_model_usage(model, False)
                        break  # Переходим к следующей модели
                        
                except Exception as e:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = handle_error(e, f"send_vision_request ({model})", show_to_user=False)
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                continue
            
            if claude_failed:
                break  # Выходим из цикла по моделям, переходим к fallback
        
        # Fallback на альтернативные модели, если все Claude модели недоступны
        # Llama 3.2 90B Vision - финальный fallback
        if claude_failed and fallback_models:
            print(f"🔄 Все Claude модели недоступны. Пробую fallback модели: {', '.join(fallback_models)}")
            for model in fallback_models:
                try:
                    start_time = time.time()
                    messages = [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": content}
                    ]
                    payload = {
                        "model": model,
                        "messages": messages,
                        "max_tokens": 1000,  # Llama обычно дешевле
                        "temperature": 0.1
                    }
                    
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        result = result_data["choices"][0]["message"]["content"]
                        
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        log_api_call(model, True, latency, None)
                        track_model_usage(model, True, tokens_used)
                        
                        model_name = self._get_model_name(model)
                        # Для документов не добавляем префикс "Медицинский анализ"
                        if is_document or (force_model and force_model.lower() == "llama"):
                            return result  # Чистый текст без префикса для документов
                        return f"**🩺 Медицинский анализ ({model_name}) [Fallback]:**\n\n{result}"
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_call(model, False, latency, error_msg)
                        track_model_usage(model, False)
                        continue
                        
                except Exception as e:
                    latency = time.time() - start_time if 'start_time' in locals() else 0
                    error_msg = handle_error(e, f"send_vision_request fallback ({model})", show_to_user=False)
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    continue
        
        return "❌ Ошибка: Все модели недоступны"
    
    def _get_model_name(self, model):
        """Получить читаемое название модели (только актуальные)"""
        model_names = {
            "anthropic/claude-opus-4.5": "Claude Opus 4.5",
            "anthropic/claude-sonnet-4.5": "Claude Sonnet 4.5",
            "anthropic/claude-haiku-4.5": "Claude Haiku 4.5",
            "meta-llama/llama-3.2-90b-vision-instruct": "Llama 3.2 90B Vision"
        }
        return model_names.get(model, model)
    
    def encode_image(self, image_array):
        """Кодирует изображение в base64 с оптимизацией для медицинских снимков"""
        if isinstance(image_array, Image.Image):
            img = image_array
        else:
            # Конвертируем numpy array
            if len(image_array.shape) == 2:
                # Grayscale
                img = Image.fromarray(image_array, mode='L')
            else:
                # RGB
                img = Image.fromarray(image_array)
        
        # Оптимизируем размер для лучшего анализа
        max_size = (1024, 1024)
        img.thumbnail(max_size, Image.Resampling.LANCZOS)
        
        buffered = io.BytesIO()
        img.save(buffered, format="PNG", optimize=True)
        img_str = base64.b64encode(buffered.getvalue()).decode()
        return img_str
    
    def send_video_request(self, prompt: str = None, video_data=None, video_path=None, metadata=None, study_type=None):
        """Анализ видео через Gemini 2.5 Flash
        
        Args:
            prompt: Промпт для анализа видео (опционально, если не указан - используется специализированный промпт по study_type)
            video_data: Видео в виде bytes (из st.file_uploader)
            video_path: Путь к видео-файлу (альтернатива video_data)
            metadata: Метаданные (опционально)
            study_type: Тип исследования ('fgds', 'colonoscopy', 'echo', 'abdominal_us', 'gynecology_us', 'mri_brain', 'mri_universal', 'chest_ct')
        
        Returns:
            Результат анализа видео
        """
        model = "google/gemini-2.5-flash"
        
        # Определяем источник видео
        video_bytes = None
        video_mime = "video/mp4"
        
        if video_data:
            video_bytes = video_data if isinstance(video_data, bytes) else video_data.read()
            # Определяем MIME-тип по расширению или содержимому
            if hasattr(video_data, 'name'):
                filename = video_data.name.lower()
                if filename.endswith('.mov'):
                    video_mime = "video/quicktime"
                elif filename.endswith('.avi'):
                    video_mime = "video/x-msvideo"
                elif filename.endswith('.webm'):
                    video_mime = "video/webm"
                elif filename.endswith('.mkv'):
                    video_mime = "video/x-matroska"
        elif video_path:
            with open(video_path, 'rb') as f:
                video_bytes = f.read()
            # Определяем MIME по расширению файла
            ext = os.path.splitext(video_path)[1].lower()
            mime_map = {
                '.mov': 'video/quicktime',
                '.avi': 'video/x-msvideo',
                '.webm': 'video/webm',
                '.mkv': 'video/x-matroska',
                '.mp4': 'video/mp4'
            }
            video_mime = mime_map.get(ext, 'video/mp4')
        else:
            return "❌ Ошибка: Не предоставлены данные видео (video_data или video_path)"
        
        if not video_bytes or len(video_bytes) == 0:
            return "❌ Ошибка: Видео-файл пуст"
        
        # Проверка размера (максимум 100MB)
        max_size = 100 * 1024 * 1024  # 100MB
        video_size_mb = len(video_bytes) / 1024 / 1024
        if len(video_bytes) > max_size:
            return f"❌ Ошибка: Размер видео превышает 100MB ({video_size_mb:.1f}MB)"
        
        # Предупреждение для больших файлов
        if video_size_mb > 50:
            import warnings
            warnings.warn(f"Большой файл ({video_size_mb:.1f}MB) - кодирование может занять время")
        
        # Кодируем видео в base64 (может занять время для больших файлов)
        try:
            video_base64 = base64.b64encode(video_bytes).decode()
        except Exception as e:
            return f"❌ Ошибка кодирования видео: {str(e)}"
        
        # Формируем промпт для видео-анализа
        # Если указан study_type, используем специализированный промпт (ленивая загрузка)
        specialized_prompt = None
        if study_type is not None and isinstance(study_type, str) and study_type.strip():
            specialized_prompt = _get_video_prompt(study_type)
        
        # ВАЖНО: Для видео Gemini использует ТОЛЬКО специализированный промпт (БЕЗ system_prompt)
        # system_prompt используется только на этапе 2 (Профессор) в send_video_request_two_stage()
        if specialized_prompt:
            # Добавляем дополнительный контекст, если передан
            context_suffix = ""
            if prompt:
                context_suffix = f"\n\nДОПОЛНИТЕЛЬНЫЙ КОНТЕКСТ:\n{prompt}"
            # ТОЛЬКО специализированный промпт, БЕЗ system_prompt
            video_prompt = f"""{specialized_prompt}{context_suffix}"""
        elif prompt:
            # Используем переданный промпт (тоже без system_prompt)
            video_prompt = f"""Ты — эксперт по анализу медицинских видео-записей (процедуры, функциональные тесты, динамические исследования).

Твоя задача — проанализировать предоставленное видео и дать подробное заключение.

Обрати внимание на:
1. **Динамические изменения:** движения, изменения состояния в процессе записи
2. **Техника выполнения процедуры:** правильность, качество, возможные ошибки
3. **Патологические изменения:** видимые отклонения от нормы в динамике
4. **Функциональные тесты:** оценка подвижности, координации, функциональных возможностей
5. **Временные характеристики:** длительность процедуры, скорость изменений

{prompt}"""
        else:
            # Базовый промпт по умолчанию (тоже без system_prompt)
            video_prompt = """Ты — эксперт по анализу медицинских видео-записей (процедуры, функциональные тесты, динамические исследования).

Твоя задача — проанализировать предоставленное видео и дать подробное заключение.

Обрати внимание на:
1. **Динамические изменения:** движения, изменения состояния в процессе записи
2. **Техника выполнения процедуры:** правильность, качество, возможные ошибки
3. **Патологические изменения:** видимые отклонения от нормы в динамике
4. **Функциональные тесты:** оценка подвижности, координации, функциональных возможностей
5. **Временные характеристики:** длительность процедуры, скорость изменений

Проанализируй предоставленное видео максимально подробно."""
        
        # Формируем контент для API в правильном формате для OpenRouter/Gemini
        # Используем формат video_url с data URI (как в примере пользователя)
        content = [
            {
                "type": "video_url",
                "video_url": {
                    "url": f"data:{video_mime};base64,{video_base64}"
                }
            },
            {
                "type": "text",
                "text": video_prompt
            }
        ]
        
        # Добавляем метаданные, если есть
        if metadata:
            metadata_str = str(metadata) if not isinstance(metadata, dict) else str(metadata)
            content.append({"type": "text", "text": f"\n\nМетаданные:\n{metadata_str}"})
        
        # Формируем запрос
        # ВАЖНО: Для видео Gemini использует ТОЛЬКО специализированный промпт (БЕЗ system_prompt)
        # system_prompt используется только на этапе 2 (Профессор) в send_video_request_two_stage()
        messages = [
            {"role": "user", "content": content}
        ]
        
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": 4000,
            "temperature": 0.1
        }
        
        try:
            start_time = time.time()
            # Уменьшаем таймаут до 120 секунд (2 минуты) для видео
            # Если видео большое, может потребоваться больше времени, но 5 минут - слишком долго
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=120)
            latency = time.time() - start_time
            
            if response.status_code == 200:
                result_data = response.json()
                result = result_data["choices"][0]["message"]["content"]
                
                # Логирование
                tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                log_api_call(model, True, latency, None)
                track_model_usage(model, True, tokens_used)
                
                return f"**🎬 Анализ видео (Gemini 2.5 Flash):**\n\n{result}"
            else:
                error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                return f"❌ Ошибка анализа видео: {error_msg}"
                
        except requests.exceptions.Timeout:
            error_msg = "Таймаут запроса (превышено 2 минуты). Видео слишком большое или API не отвечает."
            log_api_call(model, False, 120, error_msg)
            track_model_usage(model, False)
            return f"❌ Ошибка: {error_msg}\n\n💡 Попробуйте:\n- Уменьшить размер видео\n- Использовать более короткий фрагмент\n- Проверить подключение к интернету"
        except requests.exceptions.RequestException as e:
            error_msg = f"Ошибка сети: {str(e)}"
            log_api_call(model, False, 0, error_msg)
            track_model_usage(model, False)
            return f"❌ Ошибка сети: {error_msg}"
        except Exception as e:
            error_msg = handle_error(e, "send_video_request", show_to_user=False)
            log_api_call(model, False, 0, error_msg)
            track_model_usage(model, False)
            return f"❌ Ошибка при анализе видео: {error_msg}"
    
    def send_video_request_two_stage(self, prompt: str = None, video_data=None, video_path=None, 
                                     metadata=None, study_type=None):
        """
        Двухэтапный анализ видео:
        1. Этап 1: Gemini 2.5 Flash с специализированным промптом (промежуточный результат)
        2. Этап 2: Claude Opus с системным промптом профессора (итоговое заключение)
        
        Returns:
            dict: {
                'specialized': str - результат специализированного анализа (Gemini),
                'final': str - итоговое заключение от профессора (Claude Opus)
            }
        """
        # Этап 1: Специализированный анализ через Gemini
        specialized_result = self.send_video_request(
            prompt=prompt,
            video_data=video_data,
            video_path=video_path,
            metadata=metadata,
            study_type=study_type
        )
        
        # Если этап 1 не удался, возвращаем только его результат
        if specialized_result.startswith("❌"):
            return {
                'specialized': specialized_result,
                'final': None
            }
        
        # Этап 2: Итоговое заключение от профессора через Claude Opus
        # Формируем запрос для профессора на основе специализированного анализа
        professor_prompt = f"""На основе следующего специализированного анализа медицинского видео, дай итоговое клиническое заключение в формате «Клиническая директива».

СПЕЦИАЛИЗИРОВАННЫЙ АНАЛИЗ:
{specialized_result}

Твоя задача:
1. Проанализировать представленный специализированный анализ
2. Сформулировать итоговое клиническое заключение
3. Дать рекомендации по дальнейшей тактике ведения пациента
4. Указать дифференциальный диагноз и коды МКБ-10/ICD-11
5. Предоставить план действий (Step-by-Step)

ВАЖНО: Не включай логирование и таблицы веб-запросов в ответ. Только клиническое заключение."""
        
        # Используем Opus для финального заключения
        try:
            # Принудительно используем Opus для финального заключения
            models_to_try = ["anthropic/claude-opus-4.5"]
            
            for model in models_to_try:
                try:
                    start_time = time.time()
                    payload = {
                        "model": model,
                        "messages": [
                            {"role": "system", "content": self.system_prompt},
                            {"role": "user", "content": professor_prompt}
                        ],
                        "max_tokens": 8000,
                        "temperature": 0.2
                    }
                    
                    response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=300)
                    latency = time.time() - start_time
                    
                    if response.status_code == 200:
                        result_data = response.json()
                        final_result = result_data["choices"][0]["message"]["content"]
                        
                        # Логирование
                        tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                        log_api_call(model, True, latency, None)
                        track_model_usage(model, True, tokens_used)
                        
                        return {
                            'specialized': specialized_result,
                            'final': f"**🎓 Итоговое заключение (Профессор, Claude Opus 4.5):**\n\n{final_result}"
                        }
                    else:
                        error_msg = f"HTTP {response.status_code}: {response.text[:200]}"
                        log_api_call(model, False, latency, error_msg)
                        track_model_usage(model, False)
                        return {
                            'specialized': specialized_result,
                            'final': f"❌ Ошибка получения итогового заключения: {error_msg}"
                        }
                except Exception as e:
                    error_msg = handle_error(e, "send_video_request_two_stage_final", show_to_user=False)
                    return {
                        'specialized': specialized_result,
                        'final': f"❌ Ошибка при получении итогового заключения: {error_msg}"
                    }
        except Exception as e:
            error_msg = handle_error(e, "send_video_request_two_stage", show_to_user=False)
            return {
                'specialized': specialized_result,
                'final': f"❌ Ошибка на этапе финального заключения: {error_msg}"
            }
    
    def get_response(self, user_message: str, context: str = "", use_sonnet_4_5: bool = False) -> str:
        """Текстовый запрос с использованием лучшей доступной модели"""
        full_message = f"{context}\n\nВопрос: {user_message}" if context else user_message
        
        # Если запрошена модель Sonnet 4.5 для ИИ-ассистента, ставим её в приоритет
        if use_sonnet_4_5:
            models_to_try = ["anthropic/claude-sonnet-4.5"] + [m for m in self.models if m != "anthropic/claude-sonnet-4.5"]
        else:
            models_to_try = self.models
        
        # Пробуем модели по порядку
        for model in models_to_try:
            try:
                start_time = time.time()
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": full_message}
                    ],
                    "max_tokens": 8000,  # Увеличено для больших ответов
                    "temperature": 0.2
                }
                
                # Логируем размер промпта для диагностики
                prompt_size = len(full_message)
                if prompt_size > 50000:
                    print(f"⚠️ Большой промпт: {prompt_size} символов. Может потребоваться больше времени.")
                
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=300)  # Увеличен таймаут для очень длинных промптов
                latency = time.time() - start_time
                
                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]
                    
                    # Логирование
                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    log_api_call(model, True, latency, None)
                    track_model_usage(model, True, tokens_used)
                    
                    self.model = model
                    return result
                else:
                    error_msg = f"HTTP {response.status_code}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    continue
                    
            except requests.exceptions.Timeout:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = f"Таймаут запроса (>{180} секунд)"
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"⚠️ Таймаут для модели {model}")
                continue
            except Exception as e:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = handle_error(e, f"get_response ({model})", show_to_user=False)
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"⚠️ Ошибка с моделью {model}: {e}")
                continue
        
        return "❌ Ошибка: Все модели недоступны. Проверьте подключение к интернету и API ключи."

    def get_response_without_system(self, user_message: str, force_opus: bool = False) -> str:
        """
        Текстовый запрос БЕЗ глобального системного промпта (например, для узкоспециализированных ролей,
        таких как врач-генетик). Весь контекст и инструкции должны быть внутри user_message.
        """
        # Приоритет Opus для сложных клинических задач по генетике
        if force_opus:
            models_to_try = ["anthropic/claude-opus-4.5"] + [
                m for m in self.models if m != "anthropic/claude-opus-4.5"
            ]
        else:
            models_to_try = self.models

        for model in models_to_try:
            try:
                start_time = time.time()
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "user", "content": user_message}
                    ],
                    "max_tokens": 8000,
                    "temperature": 0.2,
                }

                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=300)
                latency = time.time() - start_time

                if response.status_code == 200:
                    result_data = response.json()
                    result = result_data["choices"][0]["message"]["content"]

                    tokens_used = result_data.get("usage", {}).get("total_tokens", 0)
                    log_api_call(model, True, latency, None)
                    track_model_usage(model, True, tokens_used)

                    self.model = model
                    return result
                else:
                    error_msg = f"HTTP {response.status_code}"
                    log_api_call(model, False, latency, error_msg)
                    track_model_usage(model, False)
                    continue

            except requests.exceptions.Timeout:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = "Таймаут запроса (>300 секунд)"
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"⚠️ Таймаут для модели {model}")
                continue
            except Exception as e:
                latency = time.time() - start_time if 'start_time' in locals() else 0
                error_msg = handle_error(e, f"get_response_without_system ({model})", show_to_user=False)
                log_api_call(model, False, latency, error_msg)
                track_model_usage(model, False)
                print(f"⚠️ Ошибка с моделью {model}: {e}")
                continue

        return "❌ Ошибка: Все модели недоступны для запроса без системного промпта."
    
    def general_medical_consultation(self, user_question: str) -> str:
        """Общая медицинская консультация"""
        return self.get_response(user_question)
    
    def analyze_ecg_data(self, ecg_analysis: dict, user_question: str = None) -> str:
        """Анализ ЭКГ данных с улучшенным контекстом"""
        context = f"""
📊 АВТОМАТИЧЕСКИЙ АНАЛИЗ ЭКГ:
• Частота сердечных сокращений: {ecg_analysis.get('heart_rate', 'не определена')} уд/мин
• Ритм: {ecg_analysis.get('rhythm_assessment', 'не определен')}
• Количество QRS комплексов: {ecg_analysis.get('num_beats', 'не определено')}
• Длительность записи: {ecg_analysis.get('duration', 'не определена')} с
• Качество сигнала: {ecg_analysis.get('signal_quality', 'не определено')}
"""
        
        question = user_question or """
Как врач-кардиолог, проинтерпретируйте эти данные ЭКГ:
1. Оцените показатели ритма и проводимости
2. Выявите возможные патологические изменения
3. Предложите дифференциальную диагностику
4. Дайте клинические рекомендации по дальнейшему ведению
"""
        return self.get_response(question, context)
    
    def test_connection(self):
        """Тест подключения с проверкой всех моделей (только актуальные)"""
        working_models = []
        
        # Фильтруем устаревшие модели
        active_models = [m for m in self.models if not check_deprecated(m)]
        
        for model in active_models:
            try:
                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": "Test"}],
                    "max_tokens": 5
                }
                response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=10)
                
                if response.status_code == 200:
                    model_name = self._get_model_name(model)
                    working_models.append(f"✅ {model_name}")
                    if not hasattr(self, '_best_model'):
                        self._best_model = model
                        self.model = model
                else:
                    model_name = self._get_model_name(model)
                    working_models.append(f"❌ {model_name}: {response.status_code}")
                    
            except Exception as e:
                model_name = self._get_model_name(model)
                working_models.append(f"❌ {model_name}: {str(e)}")
        
        if any("✅" in status for status in working_models):
            return True, "\n".join(["🎉 Статус моделей:"] + working_models)
        else:
            return False, "\n".join(["❌ Все модели недоступны:"] + working_models)